{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_detection_comparison.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justadudewhohacks/ipynbs/blob/master/face_detection_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxSggcBX4RrJ",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lJleH-Y4Hcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install git+https://github.com/justadudewhohacks/image_augment.py\n",
        "!pip install git+https://github.com/justadudewhohacks/colabsnippets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRWMhC4AJOnd",
        "colab_type": "text"
      },
      "source": [
        "## mxnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO4A1PFJJOHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version\n",
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zCRdbvelJRFB"
      },
      "source": [
        "## pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lFTqAXEJTY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hof5xhmLh3uf"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vh2ay-TCh3ul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "2d883b18-77ea-4cd6-91aa-2aeaf5d3d0ce"
      },
      "source": [
        "from colabsnippets.DataDownloader import DataDownloader\n",
        "\n",
        "data_downloader = DataDownloader(data_dir = './data')\n",
        "\n",
        "data_downloader.download_data({\n",
        "\t\"celeba\" : [\n",
        "    #test data starts at shard 18\n",
        "    { \"images\": \"1C19zO2dlQz4ShGFJH_zEi3ToDOH-Iomp\" },\n",
        "    { \"images\": \"1S5IIoHbNjv3p7PkJ3ItoAeQJODBX597-\" },\n",
        "    { \"images\": \"1e6TUSjz7Zv6LmDP69Yc3lRIufeK3FuAR\" }\n",
        "\t]\n",
        "}, [])\n",
        "data_downloader.drive.CreateFile({ 'id': '1rujtZzpWX5DP7E7HaxgqOPpk0cRb0ajD' }).GetContentFile('./data/celeba/landmarks.json')\n",
        "data_downloader.download_data({\n",
        "\t\"WIDER\" : [\n",
        "    { \"images\": \"1JHmXqGPngDCbM56eYPeqsaCgJC4vgL4m\", \"boxes\": \"1Hd2i-6dnaWIriFK4Hj0CLZnfGtKcKj9L\" }\n",
        "\t]\n",
        "}, ['boxes'])\n",
        "data_downloader.download_data({\n",
        "\t\"FDDB\" : [\n",
        "    { \"images\": \"1C8RpAZYg5nsPCAOLESGFwKxEaHx5V3Ag\", \"boxes\": \"1ACZPuSB7j0c0_hDIBL_MSW4rrzPhC1ab\" }\n",
        "\t]\n",
        "}, ['boxes'])\n",
        "\n",
        "print('done!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 05:52:40.180226 140364883408768 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading data for db: celeba\n",
            "downloading data for shard 0\n",
            "unzipping images.7z done in 41.83179688453674s\n",
            "downloading data for shard 1\n",
            "unzipping images.7z done in 40.39577627182007s\n",
            "downloading data for shard 2\n",
            "unzipping images.7z done in 7.8820226192474365s\n",
            "download_data - total time: 107.1499376296997s\n",
            "downloading data for db: WIDER\n",
            "downloading data for shard 0\n",
            "unzipping images.7z done in 127.27363204956055s\n",
            "unzipping boxes.7z done in 51.95311212539673s\n",
            "download_data - total time: 207.77921199798584s\n",
            "downloading data for db: FDDB\n",
            "downloading data for shard 0\n",
            "unzipping images.7z done in 5.044902324676514s\n",
            "unzipping boxes.7z done in 1.5364875793457031s\n",
            "download_data - total time: 8.680900573730469s\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CAhR3ujg912",
        "colab_type": "text"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZPw0RCag9CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import types\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from augment import ImageAugmentor, augment\n",
        "from augment.augment import abs_coords\n",
        "from colabsnippets.utils import load_json\n",
        "from colabsnippets import BatchLoader\n",
        "\n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Data Loader\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "celeba_landmarks_by_file = load_json('./data/celeba/landmarks.json')\n",
        "\n",
        "def min_bbox_from_pts(pts):\n",
        "  min_x, min_y, max_x, max_y = 1.0, 1.0, 0, 0\n",
        "  for pt in pts:\n",
        "    x, y = pt\n",
        "    min_x = x if x < min_x else min_x\n",
        "    min_y = y if y < min_y else min_y\n",
        "    max_x = max_x if x < max_x else x\n",
        "    max_y = max_y if y < max_y else y\n",
        "\n",
        "  return [min_x, min_y, max_x, max_y]\n",
        "\n",
        "def min_bbox(boxes):\n",
        "  min_x, min_y, max_x, max_y = 1.0, 1.0, 0, 0\n",
        "  for box in boxes:\n",
        "    x, y, w, h = box\n",
        "    pts = [(x, y), (x + w, y + h)]\n",
        "    for x, y in pts:\n",
        "      min_x = x if x < min_x else min_x\n",
        "      min_y = y if y < min_y else min_y\n",
        "      max_x = max_x if x < max_x else x\n",
        "      max_y = max_y if y < max_y else y\n",
        "\n",
        "  return [min_x, min_y, max_x, max_y]\n",
        " \n",
        "def json_boxes_to_array(boxes):\n",
        "  out_boxes = []\n",
        "  for box in boxes:\n",
        "    x, y, w, h = box['x'], box['y'], box['width'], box['height']\n",
        "    out_box = (x, y, w, h)     \n",
        "    if w <= 0 or h <= 0:\n",
        "      raise Exception(\"box has invalid width or height: {}\".format(out_box))   \n",
        "    for val in out_box:\n",
        "      if val < -0.5 or val > 1.5:\n",
        "        raise Exception(\"box is probably not a valid relative box: {}\".format(out_box))\n",
        "    out_boxes.append(out_box)\n",
        "  return out_boxes\n",
        " \n",
        "def extract_data_labels(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  if db == 'celeba':\n",
        "    landmarks = celeba_landmarks_by_file[img_file]\n",
        "    x, y, max_x, max_y = min_bbox_from_pts(landmarks)\n",
        "    w, h = max_x - x, max_y - y\n",
        "    padding = 1.5\n",
        "\n",
        "    x = x - (0.5 * padding * w)\n",
        "    y = y - (0.5 * padding * h)\n",
        "    w = w + (padding * w)\n",
        "    h = h + (padding * h)\n",
        "\n",
        "    return [(x, y, w, h)]\n",
        "  if db == 'WIDER' or db == 'FDDB':\n",
        "    boxes_file = img_file.replace('.jpg', '.json')\n",
        "    boxes_dir = \"boxes-shard{}\".format(data['shard']) if 'shard' in data else 'boxes'\n",
        "    boxes_path = \"./data/{}/{}/{}\".format(db, boxes_dir, boxes_file)\n",
        "    boxes = load_json(boxes_path)\n",
        "    return json_boxes_to_array(boxes)\n",
        "  \n",
        "  raise Exception(\"extract_data_labels - unknown db '{}'\".format(db))\n",
        "    \n",
        "    \n",
        "def resolve_image_path(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  img_dir = \"images-shard{}\".format(data['shard']) if 'shard' in data else 'images'\n",
        "  img_path = \"./data/{}/{}/{}\".format(db, img_dir, img_file)\n",
        "  return img_path\n",
        "\n",
        "class DataLoader(BatchLoader):\n",
        "  def __init__(self, data, image_augmentor = None, is_test = False):  \n",
        "    self.image_augmentor = image_augmentor\n",
        "    BatchLoader.__init__(\n",
        "      self, \n",
        "      data if type(data) is types.FunctionType else lambda: data, \n",
        "      resolve_image_path, \n",
        "      extract_data_labels,\n",
        "      start_epoch = None, \n",
        "      is_test = is_test\n",
        "    )\n",
        "      \n",
        "  def load_image_and_labels_batch(self, datas, image_size):\n",
        "    batch_x, batch_y = [], []\n",
        "    for data in datas:\n",
        "      image = self.load_image(data)\n",
        "      boxes = self.extract_data_labels(data)\n",
        "      if self.image_augmentor is not None:\n",
        "        roi = min_bbox(boxes)\n",
        "        image, boxes = self.image_augmentor.augment(image, boxes = boxes, random_crop = roi, pad_to_square = True, resize = image_size)\n",
        "      else:\n",
        "        image, boxes = augment(image, boxes = boxes, pad_to_square = True, resize = image_size)\n",
        "      batch_x.append(image)\n",
        "      batch_y.append(boxes)\n",
        "        \n",
        "    return batch_x, batch_y\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyfeUf7n4UwV",
        "colab_type": "text"
      },
      "source": [
        "# Retina Face\n",
        "https://github.com/deepinsight/insightface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDQcWhcg46IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/deepinsight/insightface\n",
        "!cd ./insightface/RetinaFace && make\n",
        "!wget https://www.dropbox.com/s/53ftnlarhyrpkg2/retinaface-R50.zip -O retinaface-R50.zip\n",
        "!unzip retinaface-R50.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_d6RYg55IBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import sys\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "sys.path.append('./insightface/RetinaFace')\n",
        "from insightface.RetinaFace.retinaface import RetinaFace\n",
        "\n",
        "def create_retina_face(scales = [1.0], do_flip = False):\n",
        "  detector = RetinaFace('./R50', 0, 0, 'net3')\n",
        "  def detect(img, min_score):\n",
        "    faces, landmarks = detector.detect(img, min_score, scales=scales, do_flip=do_flip)\n",
        "    out_faces = []\n",
        "    img_size = img.shape[0]\n",
        "    if faces is not None:\n",
        "      for face in faces:\n",
        "        x0, y0, x1, y1 = face.astype(np.int)[0:4] / img_size\n",
        "        out_faces.append([x0, y0, x1-x0, y1-y0])\n",
        "    return out_faces\n",
        "  return detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-6qRoMLmea",
        "colab_type": "text"
      },
      "source": [
        "# DSFD\n",
        "https://github.com/TencentYoutuResearch/FaceDetection-DSFD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QWzkY2hLoG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/TencentYoutuResearch/FaceDetection-DSFD\n",
        "DataDownloader().drive.CreateFile({ 'id': '1WeXlNYsM6dMP3xQQELI-4gxhwKUQxc3-' }).GetContentFile('./WIDERFace_DSFD_RES152.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFa7BlI5NEy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "sys.path.append('/content/FaceDetection-DSFD')\n",
        "\n",
        "from face_ssd import build_ssd\n",
        "\n",
        "def test_base_transform(image, mean):\n",
        "    x = image.astype(np.float32)\n",
        "    x -= mean\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "\n",
        "class TestBaseTransform:\n",
        "    def __init__(self, mean):\n",
        "        self.mean = np.array(mean, dtype=np.float32)\n",
        "\n",
        "    def __call__(self, image, boxes=None, labels=None):\n",
        "        return test_base_transform(image, self.mean), boxes, labels\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "def bbox_vote(det):\n",
        "  order = det[:, 4].ravel().argsort()[::-1]\n",
        "  det = det[order, :]\n",
        "  while det.shape[0] > 0:\n",
        "    # IOU\n",
        "    area = (det[:, 2] - det[:, 0] + 1) * (det[:, 3] - det[:, 1] + 1)\n",
        "    xx1 = np.maximum(det[0, 0], det[:, 0])\n",
        "    yy1 = np.maximum(det[0, 1], det[:, 1])\n",
        "    xx2 = np.minimum(det[0, 2], det[:, 2])\n",
        "    yy2 = np.minimum(det[0, 3], det[:, 3])\n",
        "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "    inter = w * h\n",
        "    o = inter / (area[0] + area[:] - inter)\n",
        "    # get needed merge det and delete these det\n",
        "    merge_index = np.where(o >= 0.3)[0]\n",
        "    det_accu = det[merge_index, :]\n",
        "    det = np.delete(det, merge_index, 0)\n",
        "    if merge_index.shape[0] <= 1:\n",
        "      continue\n",
        "    det_accu[:, 0:4] = det_accu[:, 0:4] * np.tile(det_accu[:, -1:], (1, 4))\n",
        "    max_score = np.max(det_accu[:, 4])\n",
        "    if type(max_score) == torch.Tensor:\n",
        "      max_score = np.array(max_score.cpu())\n",
        "      \n",
        "    det_accu_sum = np.zeros((1, 5))\n",
        "    s1 = np.sum(det_accu[:, -1:])\n",
        "    if type(s1) == torch.Tensor:\n",
        "      s1 = np.array(s1.cpu())\n",
        "    det_accu_sum[:, 0:4] = np.sum(det_accu[:, 0:4], axis=0) / s1\n",
        "    det_accu_sum[:, 4] = max_score\n",
        "    try:\n",
        "        dets = np.row_stack((dets, det_accu_sum))\n",
        "    except:\n",
        "        dets = det_accu_sum\n",
        "  dets = dets[0:750, :]\n",
        "  return dets\n",
        "  \n",
        "def infer(net , img , transform , thresh , shrink):\n",
        "  if shrink != 1:\n",
        "    img = cv2.resize(img, None, None, fx=shrink, fy=shrink, interpolation=cv2.INTER_LINEAR)\n",
        "    \n",
        "    \n",
        "  x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n",
        "  x = Variable(x.unsqueeze(0) , volatile=True)\n",
        "  x = x.cuda()\n",
        "  #print (shrink , x.shape)\n",
        "  y = net(x)\n",
        "  detections = y.data\n",
        "  # scale each detection back up to the image\n",
        "  scale = torch.Tensor([ img.shape[1]/shrink, img.shape[0]/shrink,\n",
        "                       img.shape[1]/shrink, img.shape[0]/shrink] )\n",
        "  det = []\n",
        "  for i in range(detections.size(1)):\n",
        "    j = 0\n",
        "    while detections[0, i, j, 0] >= thresh:\n",
        "      score = detections[0, i, j, 0]\n",
        "      #label_name = labelmap[i-1]\n",
        "      pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n",
        "      coords = (pt[0], pt[1], pt[2], pt[3]) \n",
        "      det.append([pt[0], pt[1], pt[2], pt[3], score])\n",
        "      j += 1\n",
        "  if (len(det)) == 0:\n",
        "    det = [ [0.1,0.1,0.2,0.2,0.01] ]\n",
        "  det = np.array(det)\n",
        "\n",
        "  keep_index = np.where(det[:, 4] >= 0)[0]\n",
        "  det = det[keep_index, :]\n",
        "  return det\n",
        "\n",
        "def infer_flip(net , img , transform , thresh , shrink):\n",
        "  img = cv2.flip(img, 1)\n",
        "  det = infer(net , img , transform , thresh , shrink)\n",
        "  det_t = np.zeros(det.shape)\n",
        "  det_t[:, 0] = img.shape[1] - det[:, 2]\n",
        "  det_t[:, 1] = det[:, 1]\n",
        "  det_t[:, 2] = img.shape[1] - det[:, 0]\n",
        "  det_t[:, 3] = det[:, 3]\n",
        "  det_t[:, 4] = det[:, 4]\n",
        "  return det_t\n",
        "  \n",
        "def create_dsfd(max_size = 608):\n",
        "  net = build_ssd('test', 640, 2)\n",
        "  net.load_state_dict(torch.load('./WIDERFace_DSFD_RES152.pth'))\n",
        "  net.cuda()\n",
        "  net.eval()\n",
        "\n",
        "  transform = TestBaseTransform((104, 117, 123))\n",
        "\n",
        "  def detect(img, min_score):\n",
        "    thresh = min_score\n",
        "    max_im_shrink = ( (max_size*max_size) / (img.shape[0] * img.shape[1])) ** 0.5\n",
        "    shrink = max_im_shrink if max_im_shrink < 1 else 1\n",
        "\n",
        "    det0 = infer(net, img, transform, thresh, shrink)\n",
        "    det1 = infer_flip(net , img , transform , thresh , shrink)\n",
        "    # shrink detecting and shrink only detect big face\n",
        "    st = 0.5 if max_im_shrink >= 0.75 else 0.5 * max_im_shrink\n",
        "    det_s = infer(net , img , transform , thresh , st)\n",
        "    index = np.where(np.maximum(det_s[:, 2] - det_s[:, 0] + 1, det_s[:, 3] - det_s[:, 1] + 1) > 30)[0]\n",
        "    det_s = det_s[index, :]\n",
        "    # enlarge one times\n",
        "    factor = 2\n",
        "    bt = min(factor, max_im_shrink) if max_im_shrink > 1 else (st + max_im_shrink) / 2\n",
        "    det_b = infer(net , img , transform , thresh , bt)\n",
        "    # enlarge small iamge x times for small face\n",
        "    if max_im_shrink > factor:\n",
        "        bt *= factor\n",
        "        while bt < max_im_shrink:\n",
        "            det_b = np.row_stack((det_b, infer(net , img , transform , thresh , bt)))\n",
        "            bt *= factor\n",
        "        det_b = np.row_stack((det_b, infer(net , img , transform , thresh , max_im_shrink) ))\n",
        "    # enlarge only detect small face\n",
        "    if bt > 1:\n",
        "        index = np.where(np.minimum(det_b[:, 2] - det_b[:, 0] + 1, det_b[:, 3] - det_b[:, 1] + 1) < 100)[0]\n",
        "        det_b = det_b[index, :]\n",
        "    else:\n",
        "        index = np.where(np.maximum(det_b[:, 2] - det_b[:, 0] + 1, det_b[:, 3] - det_b[:, 1] + 1) > 30)[0]\n",
        "        det_b = det_b[index, :]\n",
        "    det = np.row_stack((det0, det1, det_s, det_b))\n",
        "    det = bbox_vote(det)\n",
        "    out_faces = []\n",
        "    img_size = img.shape[0]\n",
        "    for face in det:\n",
        "      if (face[4] > min_score):\n",
        "        x0, y0, x1, y1 = face.astype(np.int)[0:4] / img_size\n",
        "        out_faces.append([x0, y0, x1-x0, y1-y0])\n",
        "    return out_faces\n",
        "  return detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmavm9cg209n",
        "colab_type": "text"
      },
      "source": [
        "# SSD Mobilenet V1\n",
        "https://github.com/yeephycho/tensorflow-face-detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cuvaxawa23z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DataDownloader().drive.CreateFile({ 'id': '0B5ttP5kO_loUdWZWZVVrN2VmWFk' }).GetContentFile('./ssd_mobilenet_v1.pb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvl7cpde3cxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_ssd_mobilenet_v1():\n",
        "  tf.reset_default_graph()\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  config.allow_soft_placement = True\n",
        "  config.log_device_placement = True\n",
        "  detection_graph = tf.Graph()\n",
        "  with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile('./ssd_mobilenet_v1.pb', 'rb') as fid:\n",
        "      serialized_graph = fid.read()\n",
        "      od_graph_def.ParseFromString(serialized_graph)\n",
        "      tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "  image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "  boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "  scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "  def detect(img, min_score):\n",
        "    with detection_graph.as_default():\n",
        "      with tf.Session(graph=detection_graph, config=config) as sess:\n",
        "        with tf.device('/gpu:0'):\n",
        "          img = np.expand_dims(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), axis=0)\n",
        "          (boxes, scores) = sess.run([boxes_tensor, scores_tensor], feed_dict={image_tensor: img})\n",
        "\n",
        "          indices = np.where(scores > min_score)\n",
        "          filtered_boxes = boxes[indices][:]\n",
        "      \n",
        "          out_boxes = []\n",
        "          for y0, x0, y1, x1 in filtered_boxes:\n",
        "            out_boxes.append(x0, y0, x1-x0, y1-y0)\n",
        "          return out_boxes\n",
        "  return detect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDyIVMhZhikg",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqtRSKp4zzmu",
        "colab_type": "text"
      },
      "source": [
        "## Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkveU4uBhkRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "dbs = ['celeba', 'fddb', 'wider']\n",
        "min_score = 0.5\n",
        "min_image_size = 128\n",
        "max_image_size =  608\n",
        "#detect = create_retina_face(scales = [1.0], do_flip = False)\n",
        "#config_name = \"retina_scales_1.0_flip_no\"\n",
        "#detect = create_ssd_mobilenet_v1()\n",
        "#config_name = \"ssd_mobilenet_v1\"\n",
        "detect = create_dsfd(608)\n",
        "config_name = \"dsfd_608\"\n",
        "  \n",
        "for db in dbs:\n",
        "  for image_size in range(min_image_size, max_image_size + 1, 32):\n",
        "    print(db, image_size)\n",
        "    out_filename = \"{}_{}_{}\".format(config_name, db, image_size)\n",
        "\n",
        "    test_data = load_json(\"./{}_testData.json\".format(db))\n",
        "    data_loader = DataLoader(test_data, is_test = True)\n",
        "    get_next_batch = lambda : data_loader.next_batch(1, image_size)\n",
        "\n",
        "    next_batch = get_next_batch()\n",
        "    results = []\n",
        "    avg_time = 0\n",
        "    while (next_batch != None):\n",
        "      batch_x, batch_gt_boxes = next_batch\n",
        "      gt_boxes = batch_gt_boxes[0]\n",
        "      ts = time()\n",
        "      boxes = detect(batch_x[0], min_score)\n",
        "      avg_time += ((time() - ts) * 1000)\n",
        "      results.append((boxes, gt_boxes))\n",
        "      next_batch = get_next_batch()\n",
        "    avg_time = avg_time / len(results)\n",
        "    print(avg_time)\n",
        "    np.save(out_filename, results, allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKt87q9U3i5f",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBvbZoM3kO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from colabsnippets.face_detection import calculate_iou\n",
        "\n",
        "min_image_size = 128\n",
        "max_image_size =  608\n",
        "#config_name = \"retina_scales_1.0_flip_no\"\n",
        "config_name = \"ssd_mobilenet_v1\"\n",
        "db = \"celeba\"\n",
        "iou_thresh = 0.5\n",
        "\n",
        "image_sizes = []\n",
        "true_positives_ratios = []\n",
        "false_positives_ratios = []\n",
        "for image_size in range(min_image_size, max_image_size + 1, 32):\n",
        "  filename = \"{}_{}_{}.npy\".format(config_name, db, image_size)\n",
        "  try:\n",
        "    results = np.load(filename, allow_pickle = True)\n",
        "  except:\n",
        "    image_sizes.append(image_size)\n",
        "    true_positives_ratios.append(-1.111)\n",
        "    false_positives_ratios.append(-1.111)\n",
        "    continue\n",
        "\n",
        "  #TODO NMS\n",
        "\n",
        "  true_positives = 0\n",
        "  false_positives = 0\n",
        "  total_boxes = 0\n",
        "  for res in results:\n",
        "    pred_boxes, gt_boxes = res\n",
        "    pred_boxes = np.array(pred_boxes) * image_size\n",
        "    gt_boxes = np.array(gt_boxes) * image_size\n",
        "    total_boxes += len(gt_boxes)\n",
        "\n",
        "    num_preds = len(pred_boxes)\n",
        "    matched_preds = np.zeros(num_preds, dtype = bool)\n",
        "    for gt_box in gt_boxes:\n",
        "      is_detected = False\n",
        "      for pred_idx, pred_box in enumerate(pred_boxes):\n",
        "        if (matched_preds[pred_idx]):\n",
        "          continue\n",
        "          \n",
        "        iou = calculate_iou(pred_box, gt_box)\n",
        "        if iou > iou_thresh:\n",
        "          matched_preds[pred_idx] = True\n",
        "          break\n",
        "\n",
        "    tp = np.sum(matched_preds)\n",
        "    fp = num_preds - tp\n",
        "    #print(len(gt_boxes), num_preds, tp, fp)\n",
        "    true_positives += tp\n",
        "    false_positives += fp\n",
        "        \n",
        "  true_positives_ratio = true_positives / total_boxes\n",
        "  false_positives_ratio = false_positives / total_boxes\n",
        "        \n",
        "  #print(filename)\n",
        "  #print(\"- true_positives: {}\".format(true_positives))\n",
        "  #print(\"- false_positives: {}\".format(false_positives))\n",
        "  #print(\"- true_positives_ratio: {}\".format(true_positives_ratio))\n",
        "  #print(\"- false_positives_ratio: {}\".format(false_positives_ratio))\n",
        "  \n",
        "  image_sizes.append(image_size)\n",
        "  true_positives_ratios.append(float(\"{:.4f}\".format(true_positives_ratio)))\n",
        "  false_positives_ratios.append(float(\"{:.4f}\".format(false_positives_ratio)))\n",
        "\n",
        "out_name = \"{}_{}_iou_{}\".format(config_name, db, iou_thresh)\n",
        "print(image_sizes)\n",
        "print(\"\\\"{}\\\": {{\".format(out_name))\n",
        "print(\"  \\\"tp\\\": {},\".format(true_positives_ratios))\n",
        "print(\"  \\\"fp\\\": {}\".format(false_positives_ratios))\n",
        "print(\"}},\".format(out_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPhlpaEal-ge",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqV_CHsp6DgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 480, 512, 544, 576, 608]\n",
        "res_iou_0_5 = {\n",
        "  \"retina_scales_1.0_flip_no_celeba_iou_0.5\": {\n",
        "    \"tp\": [0.9267, 0.9608, 0.9795, 0.9852, 0.9890, 0.9905, 0.9918, 0.9928, 0.9925, 0.9936, 0.9932, 0.9935, 0.9926, 0.9931, 0.9934, 0.9933],\n",
        "    \"fp\": [0.0192, 0.0226, 0.0271, 0.0325, 0.0377, 0.0411, 0.0452, 0.0478, 0.0531, 0.0548, 0.0590, 0.0608, 0.0631, 0.0660, 0.0680, 0.0683]\n",
        "  },\n",
        "  \"retina_scales_1.0_flip_no_fddb_iou_0.5\": {\n",
        "    \"tp\": [0.7480, 0.8122, 0.8517, 0.8731, 0.8878, 0.8994, 0.9068, 0.9151, 0.9197, 0.9240, 0.9290, 0.9313, 0.9344, 0.9339, 0.9329, 0.9341],\n",
        "    \"fp\": [0.0180, 0.0199, 0.0209, 0.0286, 0.0325, 0.0385, 0.0418, 0.0418, 0.0462, 0.0468, 0.0478, 0.0505, 0.0518, 0.0590, 0.0590, 0.0594]\n",
        "  },\n",
        "  \"retina_scales_1.0_flip_no_wider_iou_0.5\": {\n",
        "    \"tp\": [0.0237, 0.0367, 0.0532, 0.0713, 0.0915, 0.1102, 0.1311, 0.1513, 0.1726, 0.1914, 0.2097, 0.2250, 0.2430, 0.2564, 0.2697, 0.2837],\n",
        "    \"fp\": [0.0002, 0.0004, 0.0012, 0.0017, 0.0025, 0.0034, 0.0045, 0.0054, 0.0065, 0.0071, 0.0079, 0.0095, 0.0103, 0.0116, 0.0121, 0.0139]\n",
        "  },\n",
        "  \"ssd_mobilenet_v1_fddb_iou_0.5\": {\n",
        "    \"tp\": [-1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, 0.7786, 0.7799, 0.7805, 0.7861, 0.7821, 0.7844, 0.7861],\n",
        "    \"fp\": [-1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, 0.0234, 0.0232, 0.0236, 0.0248, 0.0248, 0.0242, 0.0224]\n",
        "  },\n",
        "  \"ssd_mobilenet_v1_wider_iou_0.5\": {\n",
        "    \"tp\": [-1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, 0.1677, 0.1676, 0.1679, 0.168, 0.1693, 0.1697, 0.1694],\n",
        "    \"fp\": [-1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, -1.111, 0.0116, 0.012, 0.0116, 0.0126, 0.0118, 0.0118, 0.0121]\n",
        "  },\n",
        "  \"tiny_yolo_xception_multiscale_blocks8_128_608_epoch4_fddb_iou_0.5_tp\": {\n",
        "    \"tp\": [0.3763, 0.4365, 0.4798, 0.4982, 0.4982, 0.5026, 0.4976, 0.5020, 0.4991, 0.5030, 0.5078, 0.5134, 0.5134, 0.5192, 0.5190, 0.5154],\n",
        "    \"fp\": [0.2887, 0.1887, 0.1532, 0.1158, 0.1261, 0.1228, 0.1274, 0.1360, 0.1342, 0.1309, 0.1325, 0.1321, 0.1292, 0.1425, 0.1414, 0.1371]\n",
        "  },\n",
        "  \"tiny_yolo_xception_multiscale_blocks4_128_608_epoch3_fddb_iou_0.5_tp\" {\n",
        "    \"tp\": [0.3796, 0.4367, 0.4726, 0.5013, 0.5190, 0.5160, 0.5183, 0.5287, 0.5252, 0.5272, 0.5218, 0.5258, 0.5204, 0.5123, 0.5053, 0.4941],\n",
        "    \"fp\": [0.3293, 0.2442, 0.2094, 0.1690, 0.1671, 0.1752, 0.1775, 0.1824, 0.1721, 0.1706, 0.1690, 0.1663, 0.1735, 0.1764, 0.1797, 0.1783]\n",
        "  },\n",
        "  \"tiny_yolo_xception_multiscale_blocks4_128_608_epoch3_wider_iou_0.5_tp\" {\n",
        "    \"tp\": [0.0177, 0.0222, 0.0238, 0.0243, 0.0269, 0.0256, 0.0256, 0.0267, 0.0274, 0.0278, 0.0276, 0.0279, 0.0282, 0.028, 0.0271, 0.0264],\n",
        "    \"fp\": [0.0663, 0.0617, 0.0511, 0.0434, 0.0373, 0.0325, 0.0288, 0.0257, 0.0246, 0.0204, 0.0184, 0.0162, 0.0152, 0.0145, 0.014, 0.0127]\n",
        "  },\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARsys7L1l_WH",
        "colab_type": "text"
      },
      "source": [
        "# Plot Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJW3gnPQmC0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, display\n",
        "from augment.augment import abs_coords\n",
        "import numpy as np\n",
        "\n",
        "def draw_box(img, box, color = (255, 0, 0)):\n",
        "  x, y, w, h = abs_coords(box, img)\n",
        "\n",
        "  cv2.rectangle(img, (x, y), (x + w, y + h), color, 1)\n",
        "  cv2.circle(img, (x, y), 2, (0, 0, 255), -1)\n",
        "  cv2.circle(img, (x, y + h), 2, (0, 0, 255), -1)\n",
        "  cv2.circle(img, (x + w, y), 2, (0, 0, 255), -1)\n",
        "  cv2.circle(img, (x + w, y + h), 2, (0, 0, 255), -1)\n",
        "  \n",
        "!rm -rf ./check_inputs && mkdir ./check_inputs\n",
        "\n",
        "config_name = \"retina_scales_1.0_flip_no\"\n",
        "#config_name = \"ssd_mobilenet_v1\"\n",
        "db = \"fddb\"\n",
        "image_size = 608\n",
        "display_size = 608\n",
        "start_idx = 0\n",
        "num_images = 10\n",
        "\n",
        "\n",
        "filename = \"{}_{}_{}.npy\".format(config_name, db, image_size)\n",
        "results = np.load(filename, allow_pickle = True)[start_idx : start_idx + num_images]\n",
        "test_data = load_json(\"./{}_testData.json\".format(db))[start_idx : start_idx + num_images]\n",
        "images, _ = DataLoader(test_data, is_test = True).next_batch(num_images, display_size)\n",
        "\n",
        "for idx, img in enumerate(images):\n",
        "  pred_boxes, gt_boxes = results[idx]\n",
        "  for box in pred_boxes:\n",
        "    draw_box(img, box, color = (255, 0, 0))\n",
        "  for box in gt_boxes:\n",
        "    draw_box(img, box, color = (0, 255, 0))\n",
        "  file = './check_inputs/' + str(idx) + '.jpg'\n",
        "  cv2.imwrite(file, img)\n",
        "  display(Image(file))\n",
        "    \n",
        "!rm -rf ./check_inputs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}