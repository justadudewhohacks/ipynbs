{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "age_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justadudewhohacks/ipynbs/blob/master/age_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ht-jSYnkRl3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n"
      ]
    },
    {
      "metadata": {
        "id": "6dwkETQzJjnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ye6fJEDXGlQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/justadudewhohacks/image_augment.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4nJnQmYm_2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ]
    },
    {
      "metadata": {
        "id": "e6RkL5CMhNjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "train_data_json_id = '1CDMRQdAhcws_g1yDw_29ZD5DNDDyi7Xw'\n",
        "test_data_json_id = '1_0dpT5HRTWocnK35KLQFDHzJiwV2-IQZ'\n",
        "\n",
        "utk_images_7z_id = '1c61PoUhIPKeoRzB0XDI23XMDyJaCfKSh'\n",
        "utk_landmarks_7z_id = '1Nxg7KKfEkDBWCqhusE1S6Edp6n3tTOuN'\n",
        "\n",
        "appareal_labels_json_id = '1_zfGunGuqyrftDJIEKw6NVJOS55vyOrh'\n",
        "appareal_images_7z_id = '1BDm6r88XLwDFsqOa2ZbbUtW1HDyHo5yA'\n",
        "appareal_landmarks_7z_id = '1Am36Tk-BnjfV1d8_iUpRcW-cPfQtAN0H'\n",
        "\n",
        "wiki_labels_json_id = '1BamAqN3tNEMh6kNQQ4C8nWf6gOA2IS6X'\n",
        "wiki_images_7z_id = '1Fy3pi-Pra1IsN9HDD268nRvXa1TbsryE'\n",
        "wiki_landmarks_7z_id = '1M-YeSGEEboVqNK8pTCJhbxeVaLp0TKJ4'\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "  os.makedirs('./data')\n",
        "if not os.path.exists('./data/utk'):\n",
        "  os.makedirs('./data/utk')\n",
        "if not os.path.exists('./data/appareal'):\n",
        "  os.makedirs('./data/appareal')\n",
        "if not os.path.exists('./data/wiki'):\n",
        "  os.makedirs('./data/wiki')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "    \n",
        "print('downloading trainData.json and testData.json ...')\n",
        "drive.CreateFile({ 'id': train_data_json_id }).GetContentFile('./data/trainData.json')\n",
        "drive.CreateFile({ 'id': test_data_json_id }).GetContentFile('./data/testData.json')\n",
        "\n",
        "print('downloading utk data ...')\n",
        "drive.CreateFile({ 'id': utk_images_7z_id }).GetContentFile('./data/utk/images.7z')\n",
        "drive.CreateFile({ 'id': utk_landmarks_7z_id }).GetContentFile('./data/utk/landmarks.7z')\n",
        "\n",
        "print('downloading appareal data ...')\n",
        "drive.CreateFile({ 'id': appareal_labels_json_id }).GetContentFile('./data/appareal/labels.json')\n",
        "drive.CreateFile({ 'id': appareal_images_7z_id }).GetContentFile('./data/appareal/images.7z')\n",
        "drive.CreateFile({ 'id': appareal_landmarks_7z_id }).GetContentFile('./data/appareal/landmarks.7z')\n",
        "\n",
        "print('downloading wiki data ...')\n",
        "drive.CreateFile({ 'id': wiki_labels_json_id }).GetContentFile('./data/wiki/labels.json')\n",
        "drive.CreateFile({ 'id': wiki_images_7z_id }).GetContentFile('./data/wiki/images.7z')\n",
        "drive.CreateFile({ 'id': wiki_landmarks_7z_id }).GetContentFile('./data/wiki/landmarks.7z')\n",
        "  \n",
        "print('unzipping data...')\n",
        "\n",
        "!rm -rf ./sample_data\n",
        "!cd ./data/utk && p7zip -d ./images.7z >> ../../utk-images.unzip.txt\n",
        "!cd ./data/utk && p7zip -d ./landmarks.7z >> ../../utk-landmarks.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./images.7z >> ../../appareal-images.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./landmarks.7z >> ../../appareal-landmarks.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./images.7z >> ../../wiki-images.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./landmarks.7z >> ../../wiki-landmarks.unzip.txt\n",
        "!rm -rf *.unzip.txt\n",
        "print('done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QY0yDy-HnN79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "uYpD_pQBvv7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "WbaNcbk6vr-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import google.colab as colab\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcdBul9Inxsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Weight Serialization"
      ]
    },
    {
      "metadata": {
        "id": "pD9pkq4CnxH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WeightInitializer:\n",
        "  def __init__(self, weight_initializer, bias_initializer):\n",
        "    self.weight_initializer = weight_initializer\n",
        "    self.bias_initializer = bias_initializer\n",
        "  \n",
        "  def process_batch_norm_weights(self, channels, name):\n",
        "    with tf.variable_scope(name):\n",
        "      tf.get_variable('mean', shape = [channels], initializer = tf.keras.initializers.Zeros)\n",
        "      tf.get_variable('variance', shape = [channels], initializer = tf.keras.initializers.Ones)\n",
        "      tf.get_variable('offset', shape = [channels], initializer = tf.keras.initializers.Zeros)\n",
        "      tf.get_variable('scale', shape = [channels], initializer = tf.keras.initializers.Ones)\n",
        "    \n",
        "  def process_conv_weights(self, channels_in, channels_out, name, filter_size = 3, with_batch_norm = False):\n",
        "    with tf.variable_scope(name):\n",
        "      self.weight_initializer('filter', [filter_size, filter_size, channels_in, channels_out])\n",
        "      if with_batch_norm:\n",
        "        self.process_batch_norm_weights(channels_out, 'batch_norm')\n",
        "      else:\n",
        "        self.bias_initializer('bias',[channels_out])\n",
        "    \n",
        "  def process_fc_weights(self, channels_in, channels_out, name):\n",
        "    with tf.variable_scope(name):\n",
        "      self.weight_initializer('weights', [channels_in, channels_out])\n",
        "      self.bias_initializer('bias',[channels_out])\n",
        "    \n",
        "  def process_depthwise_separable_conv2d_weights(self, channels_in, channels_out, name, with_batch_norm = False):\n",
        "    with tf.variable_scope(name):\n",
        "      self.weight_initializer('depthwise_filter', [3, 3, channels_in, 1])\n",
        "      self.weight_initializer('pointwise_filter', [1, 1, channels_in, channels_out])\n",
        "      if with_batch_norm:\n",
        "        self.process_batch_norm_weights(channels_out, 'batch_norm')\n",
        "      else:\n",
        "        self.bias_initializer('bias',[channels_out])\n",
        "\n",
        "  def process_dense_block_weights(self, channels_in, channels_out, name, is_first_layer = False, use_depthwise_separable_conv2d = True, with_batch_norm = False):\n",
        "    conv_weight_processor = self.process_depthwise_separable_conv2d_weights if use_depthwise_separable_conv2d else self.process_conv_weights\n",
        "    conv0_weight_processor = self.process_conv_weights if is_first_layer else conv_weight_processor\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "      conv0_weight_processor(channels_in, channels_out, 'conv0', with_batch_norm = with_batch_norm)\n",
        "      conv_weight_processor(channels_out, channels_out, 'conv1', with_batch_norm = with_batch_norm)\n",
        "      conv_weight_processor(channels_out, channels_out, 'conv2', with_batch_norm = with_batch_norm)\n",
        "      conv_weight_processor(channels_out, channels_out, 'conv3', with_batch_norm = with_batch_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xj5k6dBZj08I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "lbICGURqj2ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_norm(x, name):\n",
        "  with tf.variable_scope(name):\n",
        "    return tf.nn.batch_normalization(x, tf.get_variable('mean'), tf.get_variable('variance'), tf.get_variable('offset'), tf.get_variable('scale'), 1e-3)\n",
        "\n",
        "def conv2d(x, name, stride, with_batch_norm = False):\n",
        "  with tf.variable_scope(name):\n",
        "    out = tf.nn.conv2d(x, tf.get_variable('filter'), stride, 'SAME')\n",
        "    out = batch_norm(out, 'batch_norm') if with_batch_norm else tf.add(out, tf.get_variable('bias'))\n",
        "    return out\n",
        "\n",
        "def depthwise_separable_conv2d(x, name, stride, with_batch_norm = False):\n",
        "  with tf.variable_scope(name):\n",
        "    out = tf.nn.separable_conv2d(x, tf.get_variable('depthwise_filter'), tf.get_variable('pointwise_filter'), stride, 'SAME')\n",
        "    out = batch_norm(out, weights['batch_norm']) if with_batch_norm else tf.add(out, tf.get_variable('bias'))\n",
        "    return out\n",
        "  \n",
        "def fully_connected(x, name):\n",
        "  with tf.variable_scope(name):\n",
        "    weights = tf.get_variable('weights')\n",
        "    out = tf.reshape(x, [-1, weights.get_shape().as_list()[0]])\n",
        "    out = tf.matmul(out, weights)\n",
        "    out = tf.add(out, tf.get_variable('bias'))\n",
        "  return out\n",
        "\n",
        "def dense_block(x, name, is_first_layer = False, is_scale_down = True, use_depthwise_separable_conv2d = True, with_batch_norm = False):\n",
        "  conv_op = depthwise_separable_conv2d if use_depthwise_separable_conv2d else conv2d\n",
        "  initial_stride = [1, 2, 2, 1]  if is_scale_down else [1, 1, 1, 1]\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "    if is_first_layer: \n",
        "      out1 = conv2d(x, 'conv0', initial_stride, with_batch_norm = with_batch_norm) \n",
        "    else: \n",
        "      out1 = conv_op(x, 'conv0', initial_stride, with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in2 = tf.nn.relu(out1)\n",
        "    out2 = conv_op(in2, 'conv1', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in3 = tf.nn.relu(tf.add(out1, out2))\n",
        "    out3 = conv_op(in3, 'conv2', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in4 = tf.nn.relu(tf.add(out1, tf.add(out2, out3)))\n",
        "    out4 = conv_op(in4, 'conv3', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    return tf.nn.relu(tf.add(out1, tf.add(out2, tf.add(out3, out4))))\n",
        "\n",
        "def normalize(x, mean_rgb):\n",
        "  r, g, b = mean_rgb\n",
        "  shape = np.append(np.array(x.shape[0:3]), [1])\n",
        "  avg_r = tf.fill(shape, r)\n",
        "  avg_g = tf.fill(shape, g)\n",
        "  avg_b = tf.fill(shape, b)\n",
        "  avg_rgb = tf.concat([avg_r, avg_g, avg_b], 3)\n",
        "\n",
        "  return tf.divide(tf.subtract(x, avg_rgb), 255)\n",
        "\n",
        "\n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "NeuralNetwork base class\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, initialize_weights, name = None):\n",
        "    self.initialize_weights = initialize_weights\n",
        "    self.name = name\n",
        "  \n",
        "  def init_trainable_weights(self, weight_initializer = tf.keras.initializers.glorot_normal(), bias_initializer = tf.keras.initializers.Zeros()):\n",
        "    def initialize_weights_factory(initializer):\n",
        "      def initialize_weights(name, shape):\n",
        "        return tf.get_variable(name, initializer = initializer(shape))\n",
        "      \n",
        "      return initialize_weights\n",
        "      \n",
        "    self.initialize_weights(WeightInitializer(initialize_weights_factory(weight_initializer), initialize_weights_factory(bias_initializer)))\n",
        "\n",
        "  def load_weights(self, checkpoint_file, weight_initializer = tf.keras.initializers.glorot_normal(), bias_initializer = tf.keras.initializers.Zeros()):\n",
        "    checkpoint_data = np.load(checkpoint_file + '.npy')\n",
        "    meta_json = load_json(checkpoint_file + '.json')\n",
        "\n",
        "    idx = 0\n",
        "    data_idx = 0\n",
        "    \n",
        "    def initialize_weights_factory(initializer):\n",
        "      def initialize_weights(name, shape):\n",
        "        nonlocal idx, data_idx\n",
        "        if (idx >= len(meta_json)):\n",
        "          print('load_weights - warning meta_json does not contain data for idx: ' + str(idx) + ', using default initializer')\n",
        "          return tf.get_variable(name, initializer = initializer(shape))\n",
        "\n",
        "        size = 1\n",
        "        for val in shape:\n",
        "          size = size * val\n",
        "        initial_value = np.reshape(checkpoint_data[data_idx:data_idx + size], shape)\n",
        "        \n",
        "        var = tf.get_variable(name, initializer = initial_value.astype(np.float32))\n",
        "     \n",
        "        if (var.shape != meta_json[idx]['shape']):\n",
        "          raise Exception('load_weights - shapes not matching at variable ' + str(var.name) + ': ' + str(var.shape) + ' and ' + str(meta_json[idx]['shape']))\n",
        "           \n",
        "        if (var.name != meta_json[idx]['name']):\n",
        "          print('load_weights - warning: variable names not matching: ' + str(var.name) + ' and ' + str(meta_json[idx]['name']))\n",
        "        \n",
        "        idx += 1\n",
        "        data_idx += size\n",
        "\n",
        "        return var\n",
        "      \n",
        "      return initialize_weights\n",
        "    \n",
        "    self.initialize_weights(WeightInitializer(initialize_weights_factory(weight_initializer), initialize_weights_factory(bias_initializer)))\n",
        "\n",
        "      \n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "      \n",
        "class Densenet_4_4_FeatureExtractor(NeuralNetwork):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4_feature_extractor'):\n",
        "    super().__init__(self.initialize_weights, name = name)\n",
        "    self.use_depthwise_separable_conv2d = use_depthwise_separable_conv2d\n",
        "    self.with_batch_norm = with_batch_norm\n",
        "  \n",
        "  def _create_dense_block_weight_processor(self, weight_processor):\n",
        "    def _dense_block_weight_processor(cin, cout, name, is_first_layer = False):\n",
        "      return weight_processor.process_dense_block_weights(\n",
        "        cin, \n",
        "        cout, \n",
        "        name, \n",
        "        is_first_layer = is_first_layer, \n",
        "        use_depthwise_separable_conv2d = self.use_depthwise_separable_conv2d, \n",
        "        with_batch_norm = self.with_batch_norm\n",
        "      )\n",
        "    \n",
        "    return _dense_block_weight_processor\n",
        "  \n",
        "  def _create_dense_block(self):\n",
        "    def _dense_block(x, name, is_first_layer = False):\n",
        "      return dense_block(\n",
        "        x, \n",
        "        name, \n",
        "        is_first_layer = is_first_layer, \n",
        "        use_depthwise_separable_conv2d = self.use_depthwise_separable_conv2d, \n",
        "        with_batch_norm = self.with_batch_norm\n",
        "      )\n",
        "    \n",
        "    return _dense_block\n",
        "  \n",
        "  def initialize_weights(self, weight_processor, variable_scope = 'feature_extractor'):\n",
        "    process_dense_block_weights = self._create_dense_block_weight_processor(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        process_dense_block_weights(3, 32, 'dense0', is_first_layer = True)\n",
        "        process_dense_block_weights(32, 64, 'dense1')\n",
        "        process_dense_block_weights(64, 128, 'dense2')\n",
        "        process_dense_block_weights(128, 256, 'dense3')\n",
        "    \n",
        "  def forward(self, batch_tensor, variable_scope = 'feature_extractor'):\n",
        "    dense_block = self._create_dense_block()\n",
        "    \n",
        "    mean_rgb = [122.782, 117.001, 104.298]\n",
        "    normalized = normalize(batch_tensor, mean_rgb)\n",
        "    \n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        out = dense_block(normalized, 'dense0', is_first_layer = True)\n",
        "        out = dense_block(out, 'dense1')\n",
        "        out = dense_block(out, 'dense2')\n",
        "        out = dense_block(out, 'dense3')\n",
        "\n",
        "      return out  \n",
        "  \n",
        "class Densenet_4_5_FeatureExtractor(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_5_feature_extractor'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "    \n",
        "  def initialize_weights(self, weight_processor, variable_scope = 'feature_extractor'):\n",
        "    process_dense_block_weights = super()._create_dense_block_weight_processor(weight_processor)\n",
        "    \n",
        "    super().initialize_weights(weight_processor, variable_scope = variable_scope)\n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        process_dense_block_weights(256, 512, 'dense4')\n",
        "    \n",
        "  def forward(self, batch_tensor, variable_scope = 'feature_extractor'):\n",
        "    dense_block = super()._create_dense_block()\n",
        "    \n",
        "    out = super().forward(batch_tensor, variable_scope = variable_scope)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        out = dense_block(out, 'dense4')\n",
        "\n",
        "      return out\n",
        "  \n",
        "class DenseMobilenet_4_4(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm, \n",
        "      name = name\n",
        "    )\n",
        "  \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        weight_processor.process_fc_weights(256, 1, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "\n",
        "    return out\n",
        "  \n",
        "class DenseMobilenet_4_5(Densenet_4_5_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_5'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "    \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        weight_processor.process_fc_weights(512, 1, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    \n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "    \n",
        "    return out\n",
        "  \n",
        "class DenseMobilenet_4_4_DEX(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4_dex'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "  \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        weight_processor.process_fc_weights(256, 101, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "        out = tf.reduce_sum(tf.multiply(tf.nn.softmax(out), np.arange(101)), axis = 1)\n",
        "    \n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nnp9-Jwz5GqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "cCYJYkAi5Ejn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def num_in_range(val, min_val, max_val):\n",
        "  return min(max(min_val, val), max_val)\n",
        "\n",
        "def apply_intensity_adjustment(img, params):\n",
        "  has_alpha = 'alpha' in params\n",
        "  has_beta = 'beta' in params\n",
        "\n",
        "  if not has_alpha and not has_beta:\n",
        "    print('warning: intensity dict neither has alpha nor beta')\n",
        "\n",
        "  alpha = params['alpha'] if has_alpha else 1.0\n",
        "  beta = params['beta'] if has_alpha else 0.0\n",
        "\n",
        "  return np.clip((img * alpha + beta), 0, 255).astype(img.dtype)\n",
        "\n",
        "def apply_to_gray(img):\n",
        "  return cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "def apply_blur(img, params):\n",
        "  has_kernel_size = 'kernel_size' in params\n",
        "  has_std_dev = 'std_dev' in params\n",
        "\n",
        "  if not has_kernel_size and not has_std_dev:\n",
        "    print('warning: blur dict neither has kernel_size nor std_dev')\n",
        "\n",
        "  kernel_size = params['kernel_size'] if has_kernel_size else 3\n",
        "  std_dev = params['std_dev'] if has_std_dev else 1.0\n",
        "\n",
        "  return cv2.GaussianBlur(img, (kernel_size, kernel_size), std_dev, std_dev)\n",
        "\n",
        "def apply_random_crop(img, bbox):\n",
        "  height, width = img.shape[:2]\n",
        "\n",
        "  min_x, min_y, max_x, max_y = bbox\n",
        "  min_x = int(num_in_range(min_x, 0, 1) * width)\n",
        "  min_y = int(num_in_range(min_y, 0, 1) * height)\n",
        "  max_x = int(num_in_range(max_x, 0, 1) * width)\n",
        "  max_y = int(num_in_range(max_y, 0, 1) * height)\n",
        "  x0 = random.randint(0, min_x)\n",
        "  y0 = random.randint(0, min_y)\n",
        "  x1 = random.randint(0, abs(width - max_x)) + max_x\n",
        "  y1 = random.randint(0, abs(height - max_y)) + max_y\n",
        "\n",
        "  return img[y0:y1, x0:x1]\n",
        "\n",
        "def apply_rotate(img, angle):\n",
        "  height, width = img.shape[:2]\n",
        "  cx, cy = int(width / 2), int(height / 2)\n",
        "\n",
        "  M = cv2.getRotationMatrix2D((cx, cy), -angle, 1.0)\n",
        "  cos = np.abs(M[0, 0])\n",
        "  sin = np.abs(M[0, 1])\n",
        "\n",
        "  new_width = int((height * sin) + (width * cos))\n",
        "  new_height = int((height * cos) + (width * sin))\n",
        "\n",
        "  M[0, 2] += (new_width / 2) - cx\n",
        "  M[1, 2] += (new_height / 2) - cy\n",
        "\n",
        "  return cv2.warpAffine(img, M, (new_width, new_height))\n",
        "\n",
        "def augment(\n",
        "  img,\n",
        "  intensity = None, # (alpha, beta)\n",
        "  blur = None, # (kernel_size, stddev)\n",
        "  to_gray = False,\n",
        "  random_crop = None, # [(x0, y0, x1, y1)]\n",
        "  flip = False,\n",
        "  rotation_angle = None\n",
        "):\n",
        "  img = apply_intensity_adjustment(img, intensity) if intensity is not None else img\n",
        "  img = apply_blur(img, blur) if blur is not None else img\n",
        "  img = apply_to_gray(img) if to_gray is True else img\n",
        "\n",
        "  # TODO noise\n",
        "\n",
        "  img = apply_random_crop(img, random_crop) if random_crop is not None else img\n",
        "  # TODO shearing\n",
        "  img = cv2.flip(img, 1) if flip is True else img\n",
        "  img = apply_rotate(img, rotation_angle) if rotation_angle is not None else img\n",
        "\n",
        "  return img\n",
        "\n",
        "def resize_preserve_aspect_ratio(img, size):\n",
        "  height, width = img.shape[:2]\n",
        "  max_dim = max(height, width)\n",
        "  ratio = size / float(max_dim)\n",
        "  shape = (height * ratio, width * ratio)\n",
        "  resized_img = cv2.resize(img, (int(round(width * ratio)), int(round(height * ratio))))\n",
        "\n",
        "  return resized_img\n",
        "\n",
        "def pad_to_square(img):\n",
        "  if len(img.shape) == 2:\n",
        "    img = np.expand_dims(img, axis = 2)\n",
        "\n",
        "  height, width, channels = img.shape\n",
        "  max_dim = max(height, width)\n",
        "  square_img = np.zeros([max_dim, max_dim, channels], dtype = img.dtype)\n",
        "\n",
        "  dx = math.floor(abs(max_dim - width) / 2)\n",
        "  dy = math.floor(abs(max_dim - height) / 2)\n",
        "  square_img[dy:dy + height,dx:dx + width] = img\n",
        "\n",
        "  return square_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDu9_fmzP6CF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch Loader"
      ]
    },
    {
      "metadata": {
        "id": "qGgvBNtHP2L5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_json(json_file_path):\n",
        "  with open(json_file_path) as json_file:  \n",
        "    return json.load(json_file)\n",
        "\n",
        "def shuffle_array(arr):\n",
        "  arr_clone = arr[:]\n",
        "  random.shuffle(arr_clone)\n",
        "  return arr_clone\n",
        "     \n",
        "class BatchLoader:\n",
        "  def __init__(\n",
        "    self, \n",
        "    data,\n",
        "    resolve_image_path,\n",
        "    extract_data_labels,\n",
        "    augment_image = None, \n",
        "    is_test = False,\n",
        "    start_epoch = None\n",
        "  ):\n",
        "    if not is_test and start_epoch == None:\n",
        "      raise Exception('DataLoader - start_epoch has to be defined in train mode')\n",
        "    \n",
        "    self.data = data\n",
        "    self.resolve_image_path = resolve_image_path\n",
        "    self.extract_data_labels = extract_data_labels\n",
        "    self.augment_image = augment_image\n",
        "    self.is_test = is_test\n",
        "    self.epoch = start_epoch\n",
        "    self.buffered_data = shuffle_array(self.data) if not is_test else self.data\n",
        "    self.current_idx = 0\n",
        " \n",
        "  def get_end_idx(self):\n",
        "    return len(self.buffered_data)\n",
        "   \n",
        "  def load_image(self, data, image_size):\n",
        "    db = data['db']\n",
        "    img_file = data['file']\n",
        "    file_suffix = 'chip_0' if db == 'utk' else ('face_0' if db == 'appareal' else '')\n",
        "    landmarks_file = img_file.replace(file_suffix + '.jpg', file_suffix + '.json')\n",
        "    landmarks_file_path = './data/' + db + '/landmarks/' + landmarks_file\n",
        "\n",
        "    img = cv2.imread(self.resolve_image_path(data))\n",
        "    if img is None:\n",
        "      raise Exception('failed to read image from path: ' + img_file_path)\n",
        "\n",
        "    if (self.augment_image is not None):\n",
        "      img = self.augment_image(img, data)\n",
        "\n",
        "    img = pad_to_square(resize_preserve_aspect_ratio(img, image_size))\n",
        "\n",
        "    return img\n",
        "\n",
        "  def load_image_batch(self, datas, image_size):\n",
        "    preprocessed_imgs = []\n",
        "    for data in datas:\n",
        "      preprocessed_imgs.append(self.load_image(data, image_size))\n",
        "    return np.stack(preprocessed_imgs, axis = 0)\n",
        "    \n",
        "  def load_labels(self, datas):\n",
        "    labels = []\n",
        "    for data in datas:\n",
        "      labels.append(self.extract_data_labels(data))\n",
        "    return np.stack(labels, axis = 0)\n",
        "    \n",
        "  def next_batch(self, batch_size, image_size = 112):\n",
        "    if batch_size < 1:\n",
        "      raise Exception('DataLoader.next_batch - invalid batch_size: ' + str(batch_size))\n",
        "      \n",
        "    \n",
        "    from_idx = self.current_idx\n",
        "    to_idx = self.current_idx + batch_size\n",
        "    \n",
        "    # end of epoch\n",
        "    if (to_idx > len(self.buffered_data)):\n",
        "      if self.is_test:\n",
        "        to_idx = len(self.buffered_data)\n",
        "        if to_idx == self.current_idx:\n",
        "          return None\n",
        "      else:\n",
        "        self.epoch += 1\n",
        "        self.buffered_data = self.buffered_data[from_idx:] + shuffle_array(self.data)  \n",
        "        from_idx = 0\n",
        "        to_idx = batch_size\n",
        "      \n",
        "    self.current_idx = to_idx\n",
        "    \n",
        "    next_data = self.buffered_data[from_idx:to_idx]\n",
        "      \n",
        "    batch_x = self.load_image_batch(next_data, image_size)\n",
        "    batch_y = self.load_labels(next_data)\n",
        "    \n",
        "    return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-zXGSH-Qw1k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Common"
      ]
    },
    {
      "metadata": {
        "id": "vSRUcrLYQvwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "utility\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "def gpu_session(callback):\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  config.allow_soft_placement = True\n",
        "  config.log_device_placement = True\n",
        "  with tf.Session(config = config) as session:\n",
        "    with tf.device('/gpu:0'):\n",
        "      callback(session)\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "\n",
        "def download_epoch_files(start, end):\n",
        "  for epoch in range(start, end):\n",
        "    colab.files.download('epoch_' + str(epoch) + '.txt')\n",
        "    colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "    colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "    colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "def save_weights(var_list, checkpoint_file):\n",
        "  checkpoint_data = np.array([])\n",
        "  meta_data = []\n",
        "  for var in var_list:\n",
        "    meta_data.append({ 'shape': var.get_shape().as_list(), 'name': var.name })\n",
        "    checkpoint_data = np.append(checkpoint_data, var.eval().flatten())\n",
        "    \n",
        "  meta_json = open(checkpoint_file + '.json', 'w')\n",
        "  meta_json.write(json.dumps(meta_data))\n",
        "  meta_json.close()\n",
        "  np.save(checkpoint_file, checkpoint_data)\n",
        "\n",
        "\n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Data Loader\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "appareal_labels = load_json('./data/appareal/labels.json')\n",
        "wiki_labels = load_json('./data/wiki/labels.json')\n",
        "\n",
        "def extract_data_labels(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "\n",
        "  if db == 'utk':\n",
        "    age = int(float(img_file.split('_')[0]))\n",
        "    return age\n",
        "  elif db == 'appareal':\n",
        "    age = appareal_labels[img_file]['age']\n",
        "    return age\n",
        "  elif db == 'wiki':\n",
        "    age = wiki_labels[img_file]['age']\n",
        "    return age\n",
        "  else: raise('unknown db: ' + db)\n",
        "    \n",
        "def resolve_image_path(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  return './data/' + db + '/cropped-images/' + img_file   \n",
        "\n",
        "def min_bbox(landmarks):\n",
        "  min_x, min_y, max_x, max_y = 1.0, 1.0, 0, 0\n",
        "  for pt in landmarks:\n",
        "    min_x = pt['x'] if pt['x'] < min_x else min_x\n",
        "    min_y = pt['y'] if pt['y'] < min_y else min_y\n",
        "    max_x = max_x if pt['x'] < max_x else pt['x']\n",
        "    max_y = max_y if pt['y'] < max_y else pt['y']\n",
        "\n",
        "  return [min_x, min_y, max_x, max_y]\n",
        "\n",
        "def augment_image(img, data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  file_suffix = 'chip_0' if db == 'utk' else ('face_0' if db == 'appareal' else '')\n",
        "  landmarks_file = img_file.replace(file_suffix + '.jpg', file_suffix + '.json')\n",
        "  landmarks_file_path = './data/' + db + '/landmarks/' + landmarks_file\n",
        "\n",
        "  landmarks = load_json(landmarks_file_path)\n",
        "  return augment(\n",
        "    img,\n",
        "    random_crop = min_bbox(landmarks),\n",
        "    flip = random.random() < 0.5,\n",
        "    rotation_angle = random.uniform(-15, 15),\n",
        "    blur = { 'kernel_size':  random.choice([3, 5, 7, 9, 11]), 'std_dev': random.uniform(0.5, 1.5) },\n",
        "    intensity = { 'alpha': random.uniform(0.5, 1.5), 'beta': random.uniform(-20, 20) },\n",
        "    to_gray = random.random() < 0.2\n",
        "    #rotation_angle = random.uniform(-5, 5),\n",
        "    #blur = { 'kernel_size':  random.choice([3, 5, 7]), 'std_dev': random.uniform(0.8, 1.2) },\n",
        "    #intensity = { 'alpha': random.uniform(0.8, 1.2), 'beta': random.uniform(-10, 10) },\n",
        "    #to_gray = random.random() < 0.1\n",
        "  )\n",
        "\n",
        "class DataLoader(BatchLoader):\n",
        "  def __init__(self, data, with_augmentation = False, start_epoch = None, is_test = False):\n",
        "    BatchLoader.__init__(\n",
        "      self, \n",
        "      data, \n",
        "      resolve_image_path, \n",
        "      extract_data_labels, \n",
        "      augment_image = augment_image if with_augmentation else None, \n",
        "      start_epoch = start_epoch, \n",
        "      is_test = is_test\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZ_9nRRuoHuF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "p4S3cqgioJTN",
        "colab_type": "code",
        "outputId": "5627897c-73c3-4b21-b4db-51e5cae8affc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "model_name = './dense_mobilenet_4_4_dex'\n",
        "image_size = 112\n",
        "\n",
        "# training parameters\n",
        "learning_rate = 0.0001\n",
        "start_epoch = 1\n",
        "end_epoch = 2000\n",
        "batch_size = 32\n",
        "\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size])\n",
        "\n",
        "train_data = load_json('./data/trainData.json')\n",
        "data_loader = DataLoader(train_data, start_epoch = start_epoch, with_augmentation = True)\n",
        "net.init_trainable_weights()\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "log_file = open('./log.txt', 'w')\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "  \n",
        "def train(sess):\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_epoch = time.time()\n",
        "  \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  if (start_epoch != 0):\n",
        "    checkpoint = get_checkpoint(start_epoch - 1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    print('done restoring session')\n",
        "\n",
        "  while data_loader.epoch <= end_epoch:\n",
        "    epoch = data_loader.epoch\n",
        "    current_idx = data_loader.current_idx\n",
        "    end_idx = data_loader.get_end_idx()\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    batch_x, batch_y = data_loader.next_batch(batch_size, image_size)\n",
        "\n",
        "    loss, _ = sess.run([loss_op, train_op], feed_dict = { X: batch_x, Y: batch_y })\n",
        "    total_loss += loss\n",
        "    iteration_count += 1\n",
        "    log_file.write(\"epoch \" + str(epoch) + \", (\" + str(current_idx) + \" of \" + str(end_idx) + \"), loss= \" + \"{:.4f}\".format(loss) \n",
        "          + \", time= \" + str((time.time() - ts) * 1000) + \"ms \\n\")\n",
        "\n",
        "    if epoch != data_loader.epoch:\n",
        "      print('next epoch: ' + str(data_loader.epoch))\n",
        "      print('avg_loss= ' + str(total_loss / iteration_count))\n",
        "      saver.save(sess, model_name + '.ckpt', global_step = epoch)\n",
        "\n",
        "      epoch_txt_file_path = 'epoch_' + str(epoch) + '.txt'\n",
        "      epoch_txt = open(epoch_txt_file_path, 'w')\n",
        "      epoch_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "      epoch_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "      epoch_txt.write('learning_rate= ' + str(learning_rate) + '\\n')\n",
        "      epoch_txt.write('batch_size= ' + str(batch_size) + '\\n')\n",
        "      epoch_txt.write('epoch_time= ' + str(time.time() - ts_epoch) + 's \\n')\n",
        "      epoch_txt.close()\n",
        "\n",
        "      #colab.files.download(epoch_txt_file_path)\n",
        "      #colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "      total_loss = 0\n",
        "      iteration_count = 0              \n",
        "      ts_epoch = time.time()\n",
        "        \n",
        "  print('done!')\n",
        "  log_file.close() \n",
        "    \n",
        "gpu_session(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./dense_mobilenet_4_4_dex.ckpt-0\n",
            "done restoring session\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SDQqQSTTbodd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "JkzRXmBJbraO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "config.log_device_placement = True\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "model_name = './dense_mobilenet_4_4_dex'\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "\n",
        "def compile_loss_op(X, Y, weight_map):\n",
        "  age = net.forward(X, weight_map.weights)\n",
        "  loss_op = tf.losses.absolute_difference(age, Y)\n",
        "  return loss_op\n",
        "\n",
        "batch_size = 32\n",
        "dbs = ['utk', 'wiki', 'appareal']\n",
        "test_data = load_json('./data/testData.json')\n",
        "\n",
        "for epoch in range(37, -1, -1):\n",
        "  tf.reset_default_graph()\n",
        "  weight_map = net.init_trainable_weights()\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "  Y = tf.placeholder(tf.float32, [batch_size])\n",
        "  loss_op = compile_loss_op(X, Y, weight_map)\n",
        "\n",
        "  test_txt = open('test_epoch_' + str(epoch) + '.txt', 'w')\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_test = time.time()\n",
        "  #with tf.Session(tpu_address) as sess:\n",
        "  with tf.Session(config = config) as sess:\n",
        "    checkpoint = get_checkpoint(epoch)\n",
        "    sess.run(init)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    with tf.device('/gpu:0'):\n",
        "\n",
        "      total_loss_db = 0\n",
        "      iteration_count_db = 0\n",
        "      for db in dbs:\n",
        "        db_data = []\n",
        "        for data in test_data:\n",
        "          if data['db'] == db:\n",
        "            db_data.append(data)\n",
        "\n",
        "        data_loader = DataLoader(db_data, is_test = True)\n",
        "        next_batch = data_loader.next_batch(batch_size)\n",
        "        while next_batch != None:\n",
        "          batch_x, batch_y = next_batch\n",
        "          if batch_x.shape[0] != batch_size:\n",
        "            X_tmp = tf.placeholder(tf.float32, [batch_x.shape[0], 112, 112, 3])\n",
        "            Y_tmp = tf.placeholder(tf.float32, [batch_x.shape[0]])\n",
        "            loss_op_tmp = compile_loss_op(X_tmp, Y_tmp, weight_map)\n",
        "            loss = sess.run(loss_op_tmp, feed_dict = { X_tmp: batch_x, Y_tmp: batch_y }) #/ batch_x.shape[0]\n",
        "          else:\n",
        "            loss = sess.run(loss_op, feed_dict = { X: batch_x, Y: batch_y }) #/ batch_size\n",
        "          total_loss += loss\n",
        "          total_loss_db += loss\n",
        "          iteration_count += 1\n",
        "          iteration_count_db += 1\n",
        "          next_batch = data_loader.next_batch(batch_size)\n",
        "\n",
        "        print(str(db) + \", avg_loss= \" + str(total_loss_db / iteration_count_db))\n",
        "        test_txt.write(str(db) + \":\" + '\\n')\n",
        "        test_txt.write('total_loss= ' + str(total_loss_db) + '\\n')\n",
        "        test_txt.write('avg_loss= ' + str(total_loss_db / iteration_count_db) + '\\n')\n",
        "        test_txt.write('\\n')\n",
        "        total_loss_db = 0\n",
        "        iteration_count_db = 0\n",
        "\n",
        "      print(\"avg_loss= \" + str(total_loss / iteration_count))\n",
        "      test_txt.write('----------------\\n\\n')\n",
        "      test_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "      test_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "      test_txt.write('test_time= ' + str(time.time() - ts_test) + 's \\n')\n",
        "      test_txt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A7NA2g2fbEYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Overfit"
      ]
    },
    {
      "metadata": {
        "id": "QhRQWkbsbB0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "model_name = './dense_mobilenet_4_4_dex'\n",
        "image_size = 112\n",
        "\n",
        "# training parameters\n",
        "learning_rate = 0.0001\n",
        "start_epoch = 0\n",
        "end_epoch = 2000\n",
        "batch_size = 16\n",
        "\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size])\n",
        "\n",
        "train_data = load_json('./data/trainData.json')[0:batch_size]\n",
        "data_loader = DataLoader(train_data, start_epoch = start_epoch)\n",
        "net.init_trainable_weights()\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "log_file = open('./log.txt', 'w')\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "def overfit(sess):\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_epoch = time.time()\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  #saver.save(sess, model_name + '.ckpt', global_step = 0)\n",
        "  if (start_epoch != 0):\n",
        "    checkpoint = get_checkpoint(start_epoch - 1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    print('done restoring session')\n",
        " \n",
        "  while data_loader.epoch <= end_epoch:\n",
        "    epoch = data_loader.epoch\n",
        "    current_idx = data_loader.current_idx\n",
        "    end_idx = data_loader.get_end_idx()\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    batch_x, batch_y = data_loader.next_batch(batch_size, image_size)\n",
        "    loss, _ = sess.run([loss_op, train_op], feed_dict = { X: batch_x, Y: batch_y })\n",
        "    total_loss += loss\n",
        "    iteration_count += 1\n",
        "    log_file.write(\"epoch \" + str(epoch) + \", (\" + str(current_idx) + \" of \" + str(end_idx) + \"), loss= \" + \"{:.4f}\".format(loss) \n",
        "          + \", time= \" + str((time.time() - ts) * 1000) + \"ms \\n\")\n",
        "\n",
        "    if epoch != data_loader.epoch:\n",
        "      print(str(data_loader.epoch) + ': ' + \"{:.4f}\".format(total_loss / iteration_count) + \", \" + \"{:.4f}\".format(total_loss))\n",
        "      if False:\n",
        "        print('next epoch: ' + str(data_loader.epoch))\n",
        "        saver.save(sess, model_name + '.ckpt', global_step = epoch)\n",
        "\n",
        "        epoch_txt_file_path = 'epoch_' + str(epoch) + '.txt'\n",
        "        epoch_txt = open(epoch_txt_file_path, 'w')\n",
        "        epoch_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "        epoch_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "        epoch_txt.write('learning_rate= ' + str(learning_rate) + '\\n')\n",
        "        epoch_txt.write('batch_size= ' + str(batch_size) + '\\n')\n",
        "        epoch_txt.write('epoch_time= ' + str(time.time() - ts_epoch) + 's \\n')\n",
        "        epoch_txt.close()\n",
        "\n",
        "      #colab.files.download(epoch_txt_file_path)\n",
        "      #colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "      total_loss = 0\n",
        "      iteration_count = 0              \n",
        "      ts_epoch = time.time()\n",
        "\n",
        "  print('done!')\n",
        "  log_file.close() \n",
        "  \n",
        "gpu_session(overfit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxtFV1RyTl5U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transfer Weights"
      ]
    },
    {
      "metadata": {
        "id": "3uKIzch6Jr8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "net.load_weights('./feature_extractor')\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 100)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size])\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "print(len(tf.global_variables()))\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "def run1(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.save(sess, 'dense_mobilenet_4_4_dex.ckpt-0')\n",
        "\n",
        "gpu_session(run1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0_NxMGlJJYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "src_net = DenseMobilenet_4_4()\n",
        "src_net.load_weights('./feature_extractor')\n",
        "\n",
        "vars1 = []\n",
        "\n",
        "def run1(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for var in tf.global_variables():\n",
        "    vars1.append(var.eval())\n",
        "\n",
        "gpu_session(run1)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "net.init_trainable_weights()\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "vars2 = []\n",
        "\n",
        "def run2(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.restore(sess, 'dense_mobilenet_4_4_dex.ckpt-0')\n",
        "  for var in tf.global_variables():\n",
        "    vars2.append(var.eval())\n",
        "  \n",
        "gpu_session(run2) \n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "net.init_trainable_weights()\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "vars3 = []\n",
        "\n",
        "def run3(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.restore(sess, 'dense_mobilenet_4_4_dex.ckpt-0')\n",
        "  for var in tf.global_variables():\n",
        "    vars3.append(var.eval())\n",
        "  \n",
        "gpu_session(run3) \n",
        "\n",
        "print(np.array(vars1[0]) - np.array(vars3[0]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}