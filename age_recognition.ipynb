{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "age_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "uYpD_pQBvv7k",
        "Nnp9-Jwz5GqM",
        "jDu9_fmzP6CF"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justadudewhohacks/ipynbs/blob/master/age_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ht-jSYnkRl3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n"
      ]
    },
    {
      "metadata": {
        "id": "6dwkETQzJjnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4nJnQmYm_2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ]
    },
    {
      "metadata": {
        "id": "e6RkL5CMhNjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "train_data_json_id = '1CDMRQdAhcws_g1yDw_29ZD5DNDDyi7Xw'\n",
        "test_data_json_id = '1_0dpT5HRTWocnK35KLQFDHzJiwV2-IQZ'\n",
        "\n",
        "utk_images_7z_id = '1c61PoUhIPKeoRzB0XDI23XMDyJaCfKSh'\n",
        "utk_landmarks_7z_id = '1Nxg7KKfEkDBWCqhusE1S6Edp6n3tTOuN'\n",
        "\n",
        "appareal_labels_json_id = '1_zfGunGuqyrftDJIEKw6NVJOS55vyOrh'\n",
        "appareal_images_7z_id = '1BDm6r88XLwDFsqOa2ZbbUtW1HDyHo5yA'\n",
        "appareal_landmarks_7z_id = '1Am36Tk-BnjfV1d8_iUpRcW-cPfQtAN0H'\n",
        "\n",
        "wiki_labels_json_id = '1BamAqN3tNEMh6kNQQ4C8nWf6gOA2IS6X'\n",
        "wiki_images_7z_id = '1Fy3pi-Pra1IsN9HDD268nRvXa1TbsryE'\n",
        "wiki_landmarks_7z_id = '1M-YeSGEEboVqNK8pTCJhbxeVaLp0TKJ4'\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "  os.makedirs('./data')\n",
        "if not os.path.exists('./data/utk'):\n",
        "  os.makedirs('./data/utk')\n",
        "if not os.path.exists('./data/appareal'):\n",
        "  os.makedirs('./data/appareal')\n",
        "if not os.path.exists('./data/wiki'):\n",
        "  os.makedirs('./data/wiki')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "    \n",
        "print('downloading trainData.json and testData.json ...')\n",
        "drive.CreateFile({ 'id': train_data_json_id }).GetContentFile('./data/trainData.json')\n",
        "drive.CreateFile({ 'id': test_data_json_id }).GetContentFile('./data/testData.json')\n",
        "\n",
        "print('downloading utk data ...')\n",
        "drive.CreateFile({ 'id': utk_images_7z_id }).GetContentFile('./data/utk/images.7z')\n",
        "drive.CreateFile({ 'id': utk_landmarks_7z_id }).GetContentFile('./data/utk/landmarks.7z')\n",
        "\n",
        "print('downloading appareal data ...')\n",
        "drive.CreateFile({ 'id': appareal_labels_json_id }).GetContentFile('./data/appareal/labels.json')\n",
        "drive.CreateFile({ 'id': appareal_images_7z_id }).GetContentFile('./data/appareal/images.7z')\n",
        "drive.CreateFile({ 'id': appareal_landmarks_7z_id }).GetContentFile('./data/appareal/landmarks.7z')\n",
        "\n",
        "print('downloading wiki data ...')\n",
        "drive.CreateFile({ 'id': wiki_labels_json_id }).GetContentFile('./data/wiki/labels.json')\n",
        "drive.CreateFile({ 'id': wiki_images_7z_id }).GetContentFile('./data/wiki/images.7z')\n",
        "drive.CreateFile({ 'id': wiki_landmarks_7z_id }).GetContentFile('./data/wiki/landmarks.7z')\n",
        "  \n",
        "print('done!')\n",
        "\n",
        "!rm -rf ./sample_data\n",
        "!cd ./data/utk && p7zip -d ./images.7z >> ../../utk-images.unzip.txt\n",
        "!cd ./data/utk && p7zip -d ./landmarks.7z >> ../../utk-landmarks.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./images.7z >> ../../appareal-images.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./landmarks.7z >> ../../appareal-landmarks.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./images.7z >> ../../wiki-images.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./landmarks.7z >> ../../wiki-landmarks.unzip.txt\n",
        "!rm -rf *.unzip.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QY0yDy-HnN79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "uYpD_pQBvv7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "WbaNcbk6vr-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import google.colab as colab\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nnp9-Jwz5GqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "cCYJYkAi5Ejn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def num_in_range(val, min_val, max_val):\n",
        "  return min(max(min_val, val), max_val)\n",
        "\n",
        "def random_crop(img, landmarks):\n",
        "  height, width, _ = img.shape\n",
        "  min_x, min_y, max_x, max_y = width, height, 0, 0\n",
        "  for pt in landmarks:\n",
        "    min_x = pt['x'] if pt['x'] < min_x else min_x\n",
        "    min_y = pt['y'] if pt['y'] < min_y else min_y\n",
        "    max_x = max_x if pt['x'] < max_x else pt['x']\n",
        "    max_y = max_y if pt['y'] < max_y else pt['y']\n",
        "  \n",
        "  min_x = int(num_in_range(min_x, 0, 1) * width)\n",
        "  min_y = int(num_in_range(min_y, 0, 1) * height)\n",
        "  max_x = int(num_in_range(max_x, 0, 1) * width)\n",
        "  max_y = int(num_in_range(max_y, 0, 1) * height)\n",
        "  x0 = random.randint(0, min_x)\n",
        "  y0 = random.randint(0, min_y)\n",
        "  x1 = random.randint(0, abs(width - max_x)) + max_x\n",
        "  y1 = random.randint(0, abs(height - max_y)) + max_y\n",
        "\n",
        "  return img[y0:y1, x0:x1]\n",
        "\n",
        "def resize_preserve_aspect_ratio(img, size):\n",
        "  height, width, _ = img.shape\n",
        "  max_dim = max(height, width)\n",
        "  ratio = size / float(max_dim)\n",
        "  shape = (height * ratio, width * ratio)\n",
        "  resized_img = cv2.resize(img, (int(round(height * ratio)), int(round(width * ratio))))\n",
        "  \n",
        "  return resized_img\n",
        "  \n",
        "def pad_to_square(img):\n",
        "  height, width, channels = img.shape\n",
        "  max_dim = max(height, width)\n",
        "  square_img = np.zeros([max_dim, max_dim, channels])\n",
        "\n",
        "  dx = math.floor(abs(max_dim - width) / 2)\n",
        "  dy = math.floor(abs(max_dim - height) / 2)\n",
        "  square_img[dy:dy + height,dx:dx + width] = img\n",
        "\n",
        "  return square_img\n",
        "\n",
        "def preprocess(img, size, landmarks = None, with_random_crop = True):\n",
        "  cropped_img = random_crop(img, landmarks) if with_random_crop else cropped_img\n",
        "  resized_img = resize_preserve_aspect_ratio(cropped_img, size)\n",
        "  square_img = pad_to_square(resized_img)\n",
        "  \n",
        "  return square_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcdBul9Inxsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Weight Serialization"
      ]
    },
    {
      "metadata": {
        "id": "pD9pkq4CnxH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WeightProcessor:\n",
        "  def __init__(self, process_weights, processor_bias):\n",
        "    self.process_weights = process_weights\n",
        "    self.processor_bias = processor_bias\n",
        "  \n",
        "  def process_conv_weights(self, channels_in, channels_out, prefix, filter_size = 3):\n",
        "    self.process_weights([filter_size, filter_size, channels_in, channels_out], prefix + '/filter')\n",
        "    self.processor_bias([channels_out], prefix + '/bias')\n",
        "\n",
        "  def process_depthwise_separable_conv2d_weights(self, channels_in, channels_out, prefix):\n",
        "    self.process_weights([3, 3, channels_in, 1], prefix + '/depthwise_filter'),\n",
        "    self.process_weights([1, 1, channels_in, channels_out], prefix + '/pointwise_filter'),\n",
        "    self.processor_bias([channels_out], prefix + '/bias')\n",
        "\n",
        "  def process_dense_block_weights(self, channels_in, channels_out, prefix, is_first_layer = False):\n",
        "    conv0_processor = self.process_conv_weights if is_first_layer else self.process_depthwise_separable_conv2d_weights\n",
        "    conv0_processor(channels_in, channels_out, prefix + '/conv0')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_out, channels_out, prefix + '/conv1')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_out, channels_out, prefix + '/conv2')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_out, channels_out, prefix + '/conv3')\n",
        "\n",
        "  def process_bottleneck_weights(self, channels_in, channels_out, expansion_factor, prefix):\n",
        "    channels_expand = channels_in * expansion_factor\n",
        "    self.process_conv_weights(channels_in, channels_expand, prefix + '/expansion_conv', filter_size = 1)\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_expand, channels_out, prefix + '/separable_conv')\n",
        "\n",
        "class WeightMap:\n",
        "  def __init__(self, tensors, tensor_paths):\n",
        "    self.weights = {}\n",
        "    for idx, tensor in enumerate(tensors):\n",
        "      tensor_path = tensor_paths[idx]\n",
        "\n",
        "      tmp = self.weights\n",
        "      keys = tensor_path.split('/')\n",
        "      for path_idx, key in enumerate(keys):\n",
        "        is_end = path_idx == len(keys) - 1\n",
        "        tmp[key] = tensor if is_end else (tmp[key] if key in tmp else {})\n",
        "        tmp = tmp[key]\n",
        "  \n",
        "  def get_tensor_from_path(self, tensor_path):\n",
        "    tmp = self.weights\n",
        "    for key in tensor_path.split('/'):\n",
        "      tmp = tmp[key]\n",
        "    return tmp\n",
        "  \n",
        "  def set_tensor_from_path(self, tensor_path, tensor):\n",
        "    tmp = self.weights\n",
        "    path = tensor_path.split('/')\n",
        "    for key in path[0: len(path) - 1]:\n",
        "      tmp = tmp[key]\n",
        "    tmp[path[len(path) - 1]] = tensor\n",
        "    \n",
        "def init_trainable_weights(net, weight_initializer = tf.keras.initializers.glorot_normal(), bias_initializer = tf.zeros):\n",
        "  tensors = []\n",
        "  tensor_paths = []\n",
        "  def process_weights(shape, tensor_path):\n",
        "    tensors.append(tf.Variable(weight_initializer(shape)))\n",
        "    tensor_paths.append(tensor_path)\n",
        "  def process_bias(shape, tensor_path):\n",
        "    tensors.append(tf.Variable(bias_initializer(shape)))\n",
        "    tensor_paths.append(tensor_path)\n",
        "\n",
        "  net.process_weights(process_weights, process_bias)\n",
        "\n",
        "  return WeightMap(tensors, tensor_paths)\n",
        "  \n",
        "def load_weights(net, checkpoint_file):  \n",
        "  checkpoint_data = np.load(checkpoint_file)\n",
        "  \n",
        "  idx = 0\n",
        "  tensors = []\n",
        "  tensor_paths = []\n",
        "  def extract_weights_from_shape(shape, tensor_path):\n",
        "    nonlocal idx\n",
        "    size = 1\n",
        "    for val in shape:\n",
        "      size = size * val\n",
        "    tensor = tf.convert_to_tensor(np.reshape(checkpoint_data[idx:idx + size], shape), dtype=tf.float32)\n",
        "    \n",
        "    idx += size\n",
        "    tensors.append(tensor)\n",
        "    tensor_paths.append(tensor_path)\n",
        "\n",
        "  net.process_weights(extract_weights_from_shape, extract_weights_from_shape)\n",
        "\n",
        "  return WeightMap(tensors, tensor_paths)\n",
        "\n",
        "def save_weights(net, weight_map, checkpoint_file):  \n",
        "  checkpoint_data = np.array([])\n",
        "  def append_weights(shape, tensor_path):\n",
        "    nonlocal checkpoint_data\n",
        "    tensor_data_flat = weight_map.get_tensor_from_path(tensor_path).eval().flatten()\n",
        "    checkpoint_data = np.append(checkpoint_data, tensor_data_flat)\n",
        "\n",
        "  net.process_weights(append_weights, append_weights)\n",
        "  np.save(checkpoint_file, checkpoint_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xj5k6dBZj08I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "lbICGURqj2ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x, weights, stride):\n",
        "  out = tf.nn.conv2d(x, weights['filter'], stride, 'SAME')\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "\n",
        "def depthwise_separable_conv2d(x, weights, stride):\n",
        "  out = tf.nn.separable_conv2d(x, weights['depthwise_filter'], weights['pointwise_filter'], stride, 'SAME')\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "  \n",
        "def fully_connected(x, weights):\n",
        "  out = tf.reshape(x, [-1, weights['weights'].get_shape().as_list()[0]])\n",
        "  out = tf.matmul(out, weights['weights'])\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "\n",
        "def dense_block(x, weights, is_first_layer = False, is_scale_down = True):\n",
        "  initial_stride = [1, 2, 2, 1]  if is_scale_down else [1, 1, 1, 1]\n",
        "  out1 = conv2d(x, weights['conv0'], initial_stride) if is_first_layer else depthwise_separable_conv2d(x, weights['conv0'], initial_stride)\n",
        "  \n",
        "  in2 = tf.nn.relu(out1)\n",
        "  out2 = depthwise_separable_conv2d(in2, weights['conv1'], [1, 1, 1, 1])\n",
        "\n",
        "  in3 = tf.nn.relu(tf.add(out1, out2))\n",
        "  out3 = depthwise_separable_conv2d(in3, weights['conv2'], [1, 1, 1, 1])\n",
        "\n",
        "  in4 = tf.nn.relu(tf.add(out1, tf.add(out2, out3)))\n",
        "  out4 = depthwise_separable_conv2d(in4, weights['conv3'], [1, 1, 1, 1])\n",
        "\n",
        "  return tf.nn.relu(tf.add(out1, tf.add(out2, tf.add(out3, out4))))\n",
        "\n",
        "def bottleneck(x, weights, stride, is_residual = False):\n",
        "  #TODO: Relu6?\n",
        "  out = conv2d(x, weights['expansion_conv'], [1, 1, 1, 1])\n",
        "  out = depthwise_separable_conv2d(out, weights['separable_conv'], stride)\n",
        "  if is_residual:\n",
        "    out = tf.add(x, out)\n",
        "    \n",
        "  return tf.nn.relu(out)\n",
        "\n",
        "def normalize(x, mean_rgb):\n",
        "  r, g, b = mean_rgb\n",
        "  shape = np.append(np.array(x.shape[0:3]), [1])\n",
        "  avg_r = tf.fill(shape, r)\n",
        "  avg_g = tf.fill(shape, g)\n",
        "  avg_b = tf.fill(shape, b)\n",
        "  avg_rgb = tf.concat([avg_r, avg_g, avg_b], 3)\n",
        "\n",
        "  return tf.divide(tf.subtract(x, avg_rgb), 255)\n",
        "\n",
        "class DenseMobilenet_4_4:\n",
        "  def process_weights(self, process_weights, processor_bias):\n",
        "    weight_processor = WeightProcessor(process_weights, processor_bias)\n",
        "    weight_processor.process_dense_block_weights(3, 32, 'dense0', True)\n",
        "    weight_processor.process_dense_block_weights(32, 64, 'dense1')\n",
        "    weight_processor.process_dense_block_weights(64, 128, 'dense2')\n",
        "    weight_processor.process_dense_block_weights(128, 256, 'dense3')\n",
        "    weight_processor.process_weights([256, 1], 'fc_age/weights')\n",
        "    weight_processor.processor_bias([1], 'fc_age/bias')\n",
        "    \n",
        "  def forward(self, batch_tensor, weights):\n",
        "    mean_rgb = [122.782, 117.001, 104.298]\n",
        "    normalized = normalize(batch_tensor, mean_rgb)\n",
        "\n",
        "    out = dense_block(normalized, weights['dense0'], True)\n",
        "    out = dense_block(out, weights['dense1'])\n",
        "    out = dense_block(out, weights['dense2'])\n",
        "    out = dense_block(out, weights['dense3'])\n",
        "    out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "    out = fully_connected(out, weights['fc_age'])\n",
        "    \n",
        "    return out\n",
        "  \n",
        "class DenseMobilenet_4_5:\n",
        "  def process_weights(self, process_weights, processor_bias):\n",
        "    weight_processor = WeightProcessor(process_weights, processor_bias)\n",
        "    weight_processor.process_dense_block_weights(3, 32, 'dense0', True)\n",
        "    weight_processor.process_dense_block_weights(32, 64, 'dense1')\n",
        "    weight_processor.process_dense_block_weights(64, 128, 'dense2')\n",
        "    weight_processor.process_dense_block_weights(128, 256, 'dense3')\n",
        "    weight_processor.process_dense_block_weights(256, 512, 'dense4')\n",
        "    weight_processor.process_weights([512, 1], 'fc_age/weights')\n",
        "    weight_processor.processor_bias([1], 'fc_age/bias')\n",
        "    \n",
        "  def forward(self, batch_tensor, weights):\n",
        "    mean_rgb = [122.782, 117.001, 104.298]\n",
        "    normalized = normalize(batch_tensor, mean_rgb)\n",
        "\n",
        "    out = dense_block(normalized, weights['dense0'], is_first_layer = True, is_scale_down = False)\n",
        "    out = dense_block(out, weights['dense1'])\n",
        "    out = dense_block(out, weights['dense2'])\n",
        "    out = dense_block(out, weights['dense3'])\n",
        "    out = dense_block(out, weights['dense4'])\n",
        "    out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "    out = fully_connected(out, weights['fc_age'])\n",
        "    \n",
        "    return out\n",
        "  \n",
        "class MobilenetV2:\n",
        "  def process_weights(self, process_weights, processor_bias):\n",
        "    weight_processor = WeightProcessor(process_weights, processor_bias)\n",
        "    weight_processor.process_conv_weights(3, 32, 'conv_in', filter_size = 3)\n",
        "    weight_processor.process_bottleneck_weights(32, 16, 1, 'bottleneck0/n0')\n",
        "    weight_processor.process_bottleneck_weights(16, 24, 6, 'bottleneck1/n0')\n",
        "    weight_processor.process_bottleneck_weights(24, 24, 6, 'bottleneck1/n1')\n",
        "    weight_processor.process_bottleneck_weights(24, 32, 6, 'bottleneck2/n0')\n",
        "    weight_processor.process_bottleneck_weights(32, 32, 6, 'bottleneck2/n1')\n",
        "    weight_processor.process_bottleneck_weights(32, 32, 6, 'bottleneck2/n2')\n",
        "    weight_processor.process_bottleneck_weights(32, 64, 6, 'bottleneck3/n0')\n",
        "    weight_processor.process_bottleneck_weights(64, 64, 6, 'bottleneck3/n1')\n",
        "    weight_processor.process_bottleneck_weights(64, 64, 6, 'bottleneck3/n2')\n",
        "    weight_processor.process_bottleneck_weights(64, 64, 6, 'bottleneck3/n3')\n",
        "    weight_processor.process_bottleneck_weights(64, 96, 6, 'bottleneck4/n0')\n",
        "    weight_processor.process_bottleneck_weights(96, 96, 6, 'bottleneck4/n1')\n",
        "    weight_processor.process_bottleneck_weights(96, 96, 6, 'bottleneck4/n2')\n",
        "    weight_processor.process_bottleneck_weights(96, 160, 6, 'bottleneck5/n0')\n",
        "    weight_processor.process_bottleneck_weights(160, 160, 6, 'bottleneck5/n1')\n",
        "    weight_processor.process_bottleneck_weights(160, 160, 6, 'bottleneck5/n2')\n",
        "    weight_processor.process_bottleneck_weights(160, 320, 6, 'bottleneck6/n0')\n",
        "    weight_processor.process_conv_weights(320, 1280, 'conv_expand', filter_size = 1)\n",
        "    weight_processor.process_conv_weights(1280, 1, 'conv_age_out', filter_size = 1)\n",
        "    \n",
        "  def forward(self, batch_tensor, weights):\n",
        "    mean_rgb = [122.782, 117.001, 104.298]\n",
        "    normalized = normalize(batch_tensor, mean_rgb)\n",
        "\n",
        "    # initial stride of 1 (112x112 input) instead of 2 (224x224 input)\n",
        "    out = tf.nn.relu(conv2d(normalized, weights['conv_in'], [1, 1, 1, 1]))\n",
        "    out = bottleneck(out, weights['bottleneck0']['n0'], [1, 1, 1, 1])\n",
        "    out = bottleneck(out, weights['bottleneck1']['n0'], [1, 2, 2, 1])\n",
        "    out = bottleneck(out, weights['bottleneck1']['n1'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck2']['n0'], [1, 2, 2, 1])\n",
        "    out = bottleneck(out, weights['bottleneck2']['n1'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck2']['n2'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck3']['n0'], [1, 2, 2, 1])\n",
        "    out = bottleneck(out, weights['bottleneck3']['n1'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck3']['n2'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck3']['n3'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck4']['n0'], [1, 1, 1, 1])\n",
        "    out = bottleneck(out, weights['bottleneck4']['n1'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck4']['n2'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck5']['n0'], [1, 2, 2, 1])\n",
        "    out = bottleneck(out, weights['bottleneck5']['n1'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck5']['n2'], [1, 1, 1, 1], True)\n",
        "    out = bottleneck(out, weights['bottleneck6']['n0'], [1, 1, 1, 1])\n",
        "    out = tf.nn.relu(conv2d(out, weights['conv_expand'], [1, 1, 1, 1]))\n",
        "    out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "    out = tf.nn.relu(conv2d(out, weights['conv_age_out'], [1, 1, 1, 1]))\n",
        "\n",
        "    out = tf.reshape(out, [out.shape[0], out.shape[3]])\n",
        "    \n",
        "    return out\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDu9_fmzP6CF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ]
    },
    {
      "metadata": {
        "id": "qGgvBNtHP2L5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_json(json_file_path):\n",
        "  with open(json_file_path) as json_file:  \n",
        "    return json.load(json_file)\n",
        "\n",
        "def load_image(data, with_random_crop = True):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  file_suffix = 'chip_0' if db == 'utk' else ('face_0' if db == 'appareal' else '')\n",
        "  landmarks_file = img_file.replace(file_suffix + '.jpg', file_suffix + '.json')\n",
        "  img_file_path = './data/' + db + '/cropped-images/' + img_file\n",
        "  landmarks_file_path = './data/' + db + '/landmarks/' + landmarks_file\n",
        "\n",
        "  img = cv2.imread(img_file_path)\n",
        "  if img is None:\n",
        "    raise 'failed to read image from path: ' + img_file_path\n",
        "\n",
        "  landmarks = load_json(landmarks_file_path) if with_random_crop else None\n",
        "  preprocessed_img = preprocess(img, 112, landmarks, with_random_crop)\n",
        "  \n",
        "  return preprocessed_img\n",
        "    \n",
        "def load_image_batch(datas, with_random_crop = True):\n",
        "  preprocessed_imgs = []\n",
        "  for data in datas:\n",
        "    preprocessed_imgs.append(load_image(data, with_random_crop))\n",
        "  return np.stack(preprocessed_imgs, axis=0)\n",
        "\n",
        "def shuffle_array(arr):\n",
        "  arr_clone = arr[:]\n",
        "  random.shuffle(arr_clone)\n",
        "  return arr_clone\n",
        "\n",
        "class LabelExtractor:\n",
        "  def __init__(self):\n",
        "    self.appareal_labels = load_json('./data/appareal/labels.json')\n",
        "    self.wiki_labels = load_json('./data/wiki/labels.json')\n",
        "    \n",
        "  def extract_labels(self, data):\n",
        "    db = data['db']\n",
        "    img_file = data['file']\n",
        "\n",
        "    if db == 'utk':\n",
        "      age = int(float(img_file.split('_')[0]))\n",
        "      return age\n",
        "    elif db == 'appareal':\n",
        "      age = self.appareal_labels[img_file]['age']\n",
        "      return age\n",
        "    elif db == 'wiki':\n",
        "      age = self.wiki_labels[img_file]['age']\n",
        "      return age\n",
        "    else: raise('unknown db: ' + db)\n",
        "  \n",
        "class DataLoader:\n",
        "  def __init__(self, data_json, start_epoch = None, is_test = False):\n",
        "    if not is_test and start_epoch == None:\n",
        "      raise 'DataLoader - start_epoch has to be defined in train mode'\n",
        "    \n",
        "    self.label_extractor = LabelExtractor()\n",
        "    self.is_test = is_test\n",
        "    self.data = data_json\n",
        "    self.buffered_data = shuffle_array(self.data) if not is_test else self.data\n",
        "    self.current_idx = 0\n",
        "    self.epoch = start_epoch\n",
        " \n",
        "  def get_end_idx(self):\n",
        "    return len(self.buffered_data)\n",
        "    \n",
        "  def extract_all_labels(self, datas):\n",
        "    labels = []\n",
        "    for data in datas:\n",
        "      labels.append(self.label_extractor.extract_labels(data))\n",
        "    return np.expand_dims(np.stack(labels, axis = 0), axis = 1)\n",
        "    \n",
        "  def next_batch(self, batch_size):\n",
        "    if batch_size < 1:\n",
        "      raise 'DataLoader.next_batch - invalid batch_size: ' + str(batch_size)\n",
        "      \n",
        "    \n",
        "    from_idx = self.current_idx\n",
        "    to_idx = self.current_idx + batch_size\n",
        "    \n",
        "    # end of epoch\n",
        "    if (to_idx > len(self.buffered_data)):\n",
        "      if self.is_test:\n",
        "        to_idx = len(self.buffered_data)\n",
        "        if to_idx == self.current_idx:\n",
        "          return None\n",
        "      else:\n",
        "        self.epoch += 1\n",
        "        self.buffered_data = self.buffered_data[from_idx:] + shuffle_array(self.data)  \n",
        "        from_idx = 0\n",
        "        to_idx = batch_size\n",
        "      \n",
        "    self.current_idx = to_idx\n",
        "    \n",
        "    next_data = self.buffered_data[from_idx:to_idx]\n",
        "      \n",
        "    batch_x = load_image_batch(next_data)\n",
        "    batch_y = self.extract_all_labels(next_data)\n",
        "    \n",
        "    return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZ_9nRRuoHuF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "p4S3cqgioJTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "config.log_device_placement = True\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "net = MobilenetV2()\n",
        "model_name = './mobilenetv2'\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "  \n",
        "# training parameters\n",
        "learning_rate = 0.001\n",
        "start_epoch = 0\n",
        "end_epoch = 2000\n",
        "batch_size = 16\n",
        "\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, 1])\n",
        "\n",
        "train_data = load_json('./data/trainData.json')\n",
        "data_loader = DataLoader(train_data, start_epoch = start_epoch)\n",
        "weight_map = init_trainable_weights(net)\n",
        "\n",
        "age = net.forward(X, weight_map.weights)\n",
        "loss_op = tf.reduce_mean(tf.abs(tf.subtract(age, Y)))\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "log_file = open('./log.txt', 'w')\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "  \n",
        "total_loss = 0\n",
        "iteration_count = 0\n",
        "ts_epoch = time.time()\n",
        "#with tf.Session(tpu_address) as sess:\n",
        "with tf.Session(config = config) as sess:\n",
        "  sess.run(init)\n",
        "\n",
        "  if (start_epoch != 0):\n",
        "    checkpoint = get_checkpoint(start_epoch - 1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    print('done restoring session')\n",
        "    \n",
        "  with tf.device('/gpu:0'):\n",
        "      \n",
        "    while data_loader.epoch <= end_epoch:\n",
        "      epoch = data_loader.epoch\n",
        "      current_idx = data_loader.current_idx\n",
        "      end_idx = data_loader.get_end_idx()\n",
        "\n",
        "      ts = time.time()\n",
        "\n",
        "      batch_x, batch_y = data_loader.next_batch(batch_size)\n",
        "      \n",
        "      loss, _ = sess.run([loss_op, train_op], feed_dict = { X: batch_x, Y: batch_y })\n",
        "      total_loss += loss\n",
        "      iteration_count += 1\n",
        "      log_file.write(\"epoch \" + str(epoch) + \", (\" + str(current_idx) + \" of \" + str(end_idx) + \"), loss= \" + \"{:.4f}\".format(loss) \n",
        "            + \", time= \" + str((time.time() - ts) * 1000) + \"ms \\n\")\n",
        "\n",
        "      if epoch != data_loader.epoch:\n",
        "        print('next epoch: ' + str(data_loader.epoch))\n",
        "        saver.save(sess, model_name + '.ckpt', global_step = epoch)\n",
        "        \n",
        "        epoch_txt_file_path = 'epoch_' + str(epoch) + '.txt'\n",
        "        epoch_txt = open(epoch_txt_file_path, 'w')\n",
        "        epoch_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "        epoch_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "        epoch_txt.write('learning_rate= ' + str(learning_rate) + '\\n')\n",
        "        epoch_txt.write('batch_size= ' + str(batch_size) + '\\n')\n",
        "        epoch_txt.write('epoch_time= ' + str(time.time() - ts_epoch) + 's \\n')\n",
        "        epoch_txt.close()\n",
        "        \n",
        "        #colab.files.download(epoch_txt_file_path)\n",
        "        #colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "        #colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "        #colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "        total_loss = 0\n",
        "        iteration_count = 0              \n",
        "        ts_epoch = time.time()\n",
        "\n",
        "    print('done!')\n",
        "    log_file.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDQqQSTTbodd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "JkzRXmBJbraO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "config.log_device_placement = True\n",
        "\n",
        "net = DenseMobilenet_4_4()\n",
        "model_name = './dense_mobilenet_4_4'\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "\n",
        "def compile_loss_op(X, Y, weight_map):\n",
        "  age = net.forward(X, weight_map.weights)\n",
        "  loss_op = tf.reduce_sum(tf.abs(tf.subtract(age, Y)))\n",
        "  return loss_op\n",
        "\n",
        "batch_size = 32\n",
        "dbs = ['utk', 'wiki', 'appareal']\n",
        "test_data = load_json('./data/testData.json')\n",
        "\n",
        "for epoch in range(33, 120):\n",
        "  tf.reset_default_graph()\n",
        "  weight_map = init_trainable_weights(net)\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "  Y = tf.placeholder(tf.float32, [batch_size, 1])\n",
        "  loss_op = compile_loss_op(X, Y, weight_map)\n",
        "\n",
        "  test_txt = open('test_epoch_' + str(epoch) + '.txt', 'w')\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_test = time.time()\n",
        "  #with tf.Session(tpu_address) as sess:\n",
        "  with tf.Session(config = config) as sess:\n",
        "    checkpoint = get_checkpoint(epoch)\n",
        "    sess.run(init)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    with tf.device('/gpu:0'):\n",
        "\n",
        "      total_loss_db = 0\n",
        "      iteration_count_db = 0\n",
        "      for db in dbs:\n",
        "        db_data = []\n",
        "        for data in test_data:\n",
        "          if data['db'] == db:\n",
        "            db_data.append(data)\n",
        "\n",
        "        data_loader = DataLoader(db_data, is_test = True)\n",
        "        next_batch = data_loader.next_batch(batch_size)\n",
        "        while next_batch != None:\n",
        "          #print(str(db) + \" : \" + str(data_loader.current_idx) + \" of \" + str(data_loader.get_end_idx()))\n",
        "          batch_x, batch_y = next_batch\n",
        "          if batch_x.shape[0] != batch_size:\n",
        "            X_tmp = tf.placeholder(tf.float32, [batch_x.shape[0], 112, 112, 3])\n",
        "            Y_tmp = tf.placeholder(tf.float32, [batch_x.shape[0], 1])\n",
        "            loss_op_tmp = compile_loss_op(X_tmp, Y_tmp, weight_map)\n",
        "            loss = sess.run(loss_op_tmp, feed_dict = { X_tmp: batch_x, Y_tmp: batch_y }) / batch_x.shape[0]\n",
        "          else:\n",
        "            loss = sess.run(loss_op, feed_dict = { X: batch_x, Y: batch_y }) / batch_size\n",
        "          total_loss += loss\n",
        "          total_loss_db += loss\n",
        "          iteration_count += 1\n",
        "          iteration_count_db += 1\n",
        "          next_batch = data_loader.next_batch(batch_size)\n",
        "\n",
        "        test_txt.write(str(db) + \":\" + '\\n')\n",
        "        test_txt.write('total_loss= ' + str(total_loss_db) + '\\n')\n",
        "        test_txt.write('avg_loss= ' + str(total_loss_db / iteration_count_db) + '\\n')\n",
        "        test_txt.write('\\n')\n",
        "        total_loss_db = 0\n",
        "        iteration_count_db = 0\n",
        "\n",
        "      test_txt.write('----------------\\n\\n')\n",
        "      test_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "      test_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "      test_txt.write('test_time= ' + str(time.time() - ts_test) + 's \\n')\n",
        "      test_txt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}