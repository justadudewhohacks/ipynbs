{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "age_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ht-jSYnkRl3G",
        "E4nJnQmYm_2z",
        "2ZLF_BXtSdtD",
        "NJE8L6bFY-Pn",
        "Nnp9-Jwz5GqM",
        "Xj5k6dBZj08I",
        "kcdBul9Inxsw"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justadudewhohacks/ipynbs/blob/master/age_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ht-jSYnkRl3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n"
      ]
    },
    {
      "metadata": {
        "id": "6dwkETQzJjnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4nJnQmYm_2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "2ZLF_BXtSdtD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download Data"
      ]
    },
    {
      "metadata": {
        "id": "e6RkL5CMhNjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "utk_images_7z_id = ''\n",
        "utk_landmarks_7z_id = ''\n",
        "\n",
        "appareal_labels_json_id = ''\n",
        "appareal_images_7z_id = ''\n",
        "appareal_landmarks_7z_id = ''\n",
        "\n",
        "wiki_labels_json_id = ''\n",
        "wiki_images_7z_id = ''\n",
        "wiki_landmarks_7z_id = ''\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "  os.makedirs('./data')\n",
        "if not os.path.exists('./data/utk'):\n",
        "  os.makedirs('./data/utk')\n",
        "if not os.path.exists('./data/appareal'):\n",
        "  os.makedirs('./data/appareal')\n",
        "if not os.path.exists('./data/wiki'):\n",
        "  os.makedirs('./data/wiki')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "  \n",
        "print('downloading utk data...')\n",
        "drive.CreateFile({ 'id': utk_images_7z_id }).GetContentFile('./data/utk/images.7z')\n",
        "drive.CreateFile({ 'id': utk_landmarks_7z_id }).GetContentFile('./data/utk/landmarks.7z')\n",
        "\n",
        "print('downloading appareal data...')\n",
        "drive.CreateFile({ 'id': appareal_labels_json_id }).GetContentFile('./data/appareal/labels.json')\n",
        "drive.CreateFile({ 'id': appareal_images_7z_id }).GetContentFile('./data/appareal/images.7z')\n",
        "drive.CreateFile({ 'id': appareal_landmarks_7z_id }).GetContentFile('./data/appareal/landmarks.7z')\n",
        "\n",
        "print('downloading wiki data...')\n",
        "drive.CreateFile({ 'id': wiki_labels_json_id }).GetContentFile('./data/wiki/labels.json')\n",
        "drive.CreateFile({ 'id': wiki_images_7z_id }).GetContentFile('./data/wiki/images.7z')\n",
        "drive.CreateFile({ 'id': wiki_landmarks_7z_id }).GetContentFile('./data/wiki/landmarks.7z')\n",
        "  \n",
        "print('done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJE8L6bFY-Pn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Unzip Data"
      ]
    },
    {
      "metadata": {
        "id": "d1rf140N_pmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd ./data/utk && p7zip -d ./images.7z\n",
        "!cd ./data/utk && p7zip -d ./landmarks.7z\n",
        "!cd ./data/appareal && p7zip -d ./images.7z\n",
        "!cd ./data/appareal && p7zip -d ./landmarks.7z\n",
        "!cd ./data/wiki && p7zip -d ./images.7z\n",
        "!cd ./data/wiki && p7zip -d ./landmarks.7z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QY0yDy-HnN79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "Nnp9-Jwz5GqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "cCYJYkAi5Ejn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "from random import randint\n",
        "\n",
        "def num_in_range(val, min_val, max_val):\n",
        "  return min(max(min_val, val), max_val)\n",
        "\n",
        "def random_crop(img, landmarks):\n",
        "  height, width, _ = img.shape\n",
        "  min_x, min_y, max_x, max_y = width, height, 0, 0\n",
        "  for pt in landmarks:\n",
        "    min_x = pt['x'] if pt['x'] < min_x else min_x\n",
        "    min_y = pt['y'] if pt['y'] < min_y else min_y\n",
        "    max_x = max_x if pt['x'] < max_x else pt['x']\n",
        "    max_y = max_y if pt['y'] < max_y else pt['y']\n",
        "  \n",
        "  min_x = int(num_in_range(min_x, 0, 1) * width)\n",
        "  min_y = int(num_in_range(min_y, 0, 1) * height)\n",
        "  max_x = int(num_in_range(max_x, 0, 1) * width)\n",
        "  max_y = int(num_in_range(max_y, 0, 1) * height)\n",
        "  x0 = randint(0, min_x)\n",
        "  y0 = randint(0, min_y)\n",
        "  x1 = randint(0, abs(width - max_x)) + max_x\n",
        "  y1 = randint(0, abs(height - max_y)) + max_y\n",
        "\n",
        "  return img[y0:y1, x0:x1]\n",
        "\n",
        "def resize_preserve_aspect_ratio(img, size):\n",
        "  height, width, _ = img.shape\n",
        "  max_dim = max(height, width)\n",
        "  ratio = size / float(max_dim)\n",
        "  shape = (height * ratio, width * ratio)\n",
        "  resized_img = cv2.resize(img, (int(round(height * ratio)), int(round(width * ratio))))\n",
        "  \n",
        "  return resized_img\n",
        "  \n",
        "def pad_to_square(img):\n",
        "  height, width, channels = img.shape\n",
        "  max_dim = max(height, width)\n",
        "  square_img = np.zeros([max_dim, max_dim, channels])\n",
        "\n",
        "  dx = math.floor(abs(max_dim - width) / 2)\n",
        "  dy = math.floor(abs(max_dim - height) / 2)\n",
        "  square_img[dy:dy + height,dx:dx + width] = img\n",
        "\n",
        "  return square_img\n",
        "\n",
        "def preprocess(img, landmarks, size):\n",
        "  cropped_img = random_crop(img, landmarks)\n",
        "  resized_img = resize_preserve_aspect_ratio(cropped_img, size)\n",
        "  square_img = pad_to_square(resized_img)\n",
        "  \n",
        "  return square_img\n",
        "\n",
        "def load_batch(datas):\n",
        "  preprocessed_imgs = []\n",
        "  \n",
        "  for data in datas:\n",
        "    db = data['db']\n",
        "    img_file = data['file']\n",
        "    file_suffix = 'chip_0' if db == 'utk' else ('face_0' if db == 'appareal' else '')\n",
        "    landmarks_file = img_file.replace(file_suffix + '.jpg', file_suffix + '.json')\n",
        "    img_file_path = './data/' + db + '/cropped-images/' + img_file\n",
        "    landmarks_file_path = './data/' + db + '/landmarks/' + landmarks_file\n",
        "    \n",
        "    img = cv2.imread(img_file_path)\n",
        "    with open(landmarks_file_path) as json_file:  \n",
        "      landmarks = json.load(json_file)\n",
        "      preprocessed_img = preprocess(img, landmarks, 112)\n",
        "      preprocessed_imgs.append(preprocessed_img)\n",
        "      \n",
        "  return np.stack(preprocessed_imgs, axis=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xj5k6dBZj08I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "lbICGURqj2ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def conv2d(x, weights, stride):\n",
        "  out = tf.nn.conv2d(x, weights['filter'], stride, 'same')\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "\n",
        "def depthwise_separable_conv2d(x, weights, stride):\n",
        "  out = tf.nn.separable_conv2d(x, params['depthwise_filter'], params['pointwise_filter'], stride, 'same')\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "  \n",
        "def fully_connected(x, weights):\n",
        "  out = tf.reshape(x, [-1, weights['weights'].get_shape().as_list()[0]])\n",
        "  out = tf.matmul(out, weights['weights'])\n",
        "  out = tf.add(out, weights['bias'])\n",
        "  return out\n",
        "\n",
        "def dense_block(x, weights, is_first_layer = False, is_scale_down = True):\n",
        "  initial_stride = [2, 2]  if is_scale_down else [1, 1]\n",
        "  out1 = conv2d(x, weights['conv0'], initial_stride) if is_first_layer else depthwise_separable_conv2d(x, weights['conv0'], initial_stride)\n",
        "  \n",
        "  in2 = tf.nn.relu(out1)\n",
        "  out2 = depthwise_separable_conv2d(in2, weights['conv1'], [1, 1])\n",
        "\n",
        "  in3 = tf.nn.relu(tf.add(out1, out2))\n",
        "  out3 = depthwise_separable_conv2d(in3, weights['conv2'], [1, 1])\n",
        "\n",
        "  in4 = tf.nn.relu(tf.add(out1, tf.add(out2, out3)))\n",
        "  out4 = depthwise_separable_conv2d(in4, weights['conv3'], [1, 1])\n",
        "\n",
        "  return tf.nn.relu(tf.add(out1, tf.add(out2, tf.add(out3, out4))))\n",
        "\n",
        "\n",
        "def forward(batch_tensor, weights):\n",
        "  mean_rgb = [122.782, 117.001, 104.298]\n",
        "  normalized = tf.div(normalize(batch_tensor, mean_rgb), tf.scalar(255))\n",
        "\n",
        "  out = dense_block(normalized, weights['dense0'], true)\n",
        "  out = dense_block(out, weights['dense1'])\n",
        "  out = dense_block(out, weights['dense2'])\n",
        "  out = dense_block(out, weights['dense3'])\n",
        "  out = tf.nn.avg_pool(out, [7, 7], [2, 2], 'valid')\n",
        "  out = fully_connected(out, weights['fc_age'])\n",
        "\n",
        "  return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcdBul9Inxsw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Weight Serialization"
      ]
    },
    {
      "metadata": {
        "id": "pD9pkq4CnxH5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WeightProcessor:\n",
        "  def __init__(self, process_weights, processor_bias):\n",
        "    self.process_weights = process_weights\n",
        "    self.processor_bias = processor_bias\n",
        "  \n",
        "  def process_conv_weights(self, channels_in, channels_out, prefix):\n",
        "    self.process_weights([3, 3, channels_in, channels_out], prefix + '/filter')\n",
        "    self.processor_bias([channels_out], prefix + '/bias')\n",
        "\n",
        "  def process_depthwise_separable_conv2d_weights(self, channels_in, channels_out, prefix):\n",
        "    self.process_weights([3, 3, channels_in, 1], prefix + '/depthwise_filter'),\n",
        "    self.process_weights([1, 1, channels_in, channels_out], prefix + '/pointwise_filter'),\n",
        "    self.processor_bias([channels_out], prefix + '/bias')\n",
        "\n",
        "  def process_dense_block_weights(self, channels_in, channels_out, prefix, is_first_layer = False):\n",
        "    conv0_processor = self.process_conv_weights if is_first_layer else self.process_depthwise_separable_conv2d_weights\n",
        "    conv0_processor(channels_in, channels_out, prefix + '/conv0')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_in, channels_out, prefix + '/conv1')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_in, channels_out, prefix + '/conv2')\n",
        "    self.process_depthwise_separable_conv2d_weights(channels_in, channels_out, prefix + '/conv3')\n",
        "\n",
        "  def process(self):\n",
        "    self.process_dense_block_weights(3, 32, 'dense0', True)\n",
        "    self.process_dense_block_weights(32, 64, 'dense1')\n",
        "    self.process_dense_block_weights(64, 128, 'dense2')\n",
        "    self.process_dense_block_weights(128, 256, 'dense3')\n",
        "    self.process_weights([128, 1], 'fc_age/weights')\n",
        "    self.processor_bias([1], 'fc_age/bias')\n",
        "\n",
        "def build_weight_map(tensors, tensor_paths):\n",
        "  weights = {}\n",
        "  for idx, tensor in enumerate(tensors):\n",
        "    tensor_path = tensor_paths[idx]\n",
        "    \n",
        "    tmp = weights\n",
        "    keys = tensor_path.split('/')\n",
        "    for path_idx, key in enumerate(keys):\n",
        "      is_end = path_idx == len(keys) - 1\n",
        "      tmp[key] = tensor if is_end else (tmp[key] if key in tmp else {})\n",
        "      tmp = tmp[key]\n",
        "      \n",
        "  return weights\n",
        "  \n",
        "    \n",
        "def init_weights(weight_initializer = tf.keras.initializers.glorot_normal(), bias_initializer = tf.zeros):\n",
        "  tensors = []\n",
        "  tensor_paths = []\n",
        "  def process_weights(shape, tensor_path):\n",
        "    tensors.append(weight_initializer(shape))\n",
        "    tensor_paths.append(tensor_path)\n",
        "  def process_bias(shape, tensor_path):\n",
        "    tensors.append(bias_initializer(shape))\n",
        "    tensor_paths.append(tensor_path)\n",
        "\n",
        "  WeightProcessor(process_weights, process_bias).process()\n",
        "\n",
        "  return build_weight_map(tensors, tensor_paths)\n",
        "  \n",
        "def load_weights(checkpoint_file):  \n",
        "  checkpoint_data = np.load(checkpoint_file)\n",
        "  \n",
        "  idx = 0\n",
        "  tensors = []\n",
        "  tensor_paths = []\n",
        "  def extract_weights_from_shape(shape, tensor_path):\n",
        "    nonlocal idx\n",
        "    size = 1\n",
        "    for val in shape:\n",
        "      size = size * val\n",
        "    tensor = tf.convert_to_tensor(np.reshape(checkpoint_data[idx:idx + size], shape), dtype=tf.float32)\n",
        "    \n",
        "    idx += size\n",
        "    tensors.append(tensor)\n",
        "    tensor_paths.append(tensor_path)\n",
        "\n",
        "  WeightProcessor(extract_weights_from_shape, extract_weights_from_shape).process()\n",
        "\n",
        "  return build_weight_map(tensors, tensor_paths)\n",
        "\n",
        "def save_weights(checkpoint_file, weights):  \n",
        "  checkpoint_data = np.array([])\n",
        "  def append_weights(shape, tensor_path):\n",
        "    nonlocal checkpoint_data\n",
        "    tmp = weights\n",
        "    for key in tensor_path.split('/'):\n",
        "      tmp = tmp[key]\n",
        "    tensor_data_flat = tmp.eval().flatten()\n",
        "    checkpoint_data = np.append(checkpoint_data, tensor_data_flat)\n",
        "    \n",
        "  WeightProcessor(append_weights, append_weights).process()\n",
        "  np.save(checkpoint_file, checkpoint_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZ_9nRRuoHuF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "p4S3cqgioJTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}