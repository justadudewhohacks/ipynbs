{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "age_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justadudewhohacks/ipynbs/blob/master/age_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "E4nJnQmYm_2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Data"
      ]
    },
    {
      "metadata": {
        "id": "e6RkL5CMhNjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "train_data_json_id = '1CDMRQdAhcws_g1yDw_29ZD5DNDDyi7Xw'\n",
        "test_data_json_id = '1_0dpT5HRTWocnK35KLQFDHzJiwV2-IQZ'\n",
        "\n",
        "utk_images_7z_id = '1c61PoUhIPKeoRzB0XDI23XMDyJaCfKSh'\n",
        "utk_landmarks_7z_id = '1Nxg7KKfEkDBWCqhusE1S6Edp6n3tTOuN'\n",
        "\n",
        "appareal_labels_json_id = '1_zfGunGuqyrftDJIEKw6NVJOS55vyOrh'\n",
        "appareal_images_7z_id = '1BDm6r88XLwDFsqOa2ZbbUtW1HDyHo5yA'\n",
        "appareal_landmarks_7z_id = '1Am36Tk-BnjfV1d8_iUpRcW-cPfQtAN0H'\n",
        "\n",
        "wiki_labels_json_id = '1BamAqN3tNEMh6kNQQ4C8nWf6gOA2IS6X'\n",
        "wiki_images_7z_id = '1Fy3pi-Pra1IsN9HDD268nRvXa1TbsryE'\n",
        "wiki_landmarks_7z_id = '1M-YeSGEEboVqNK8pTCJhbxeVaLp0TKJ4'\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "  os.makedirs('./data')\n",
        "if not os.path.exists('./data/utk'):\n",
        "  os.makedirs('./data/utk')\n",
        "if not os.path.exists('./data/appareal'):\n",
        "  os.makedirs('./data/appareal')\n",
        "if not os.path.exists('./data/wiki'):\n",
        "  os.makedirs('./data/wiki')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "    \n",
        "print('downloading trainData.json and testData.json ...')\n",
        "drive.CreateFile({ 'id': train_data_json_id }).GetContentFile('./data/trainData.json')\n",
        "drive.CreateFile({ 'id': test_data_json_id }).GetContentFile('./data/testData.json')\n",
        "\n",
        "print('downloading utk data ...')\n",
        "drive.CreateFile({ 'id': utk_images_7z_id }).GetContentFile('./data/utk/images.7z')\n",
        "drive.CreateFile({ 'id': utk_landmarks_7z_id }).GetContentFile('./data/utk/landmarks.7z')\n",
        "\n",
        "print('downloading appareal data ...')\n",
        "drive.CreateFile({ 'id': appareal_labels_json_id }).GetContentFile('./data/appareal/labels.json')\n",
        "drive.CreateFile({ 'id': appareal_images_7z_id }).GetContentFile('./data/appareal/images.7z')\n",
        "drive.CreateFile({ 'id': appareal_landmarks_7z_id }).GetContentFile('./data/appareal/landmarks.7z')\n",
        "\n",
        "print('downloading wiki data ...')\n",
        "drive.CreateFile({ 'id': wiki_labels_json_id }).GetContentFile('./data/wiki/labels.json')\n",
        "drive.CreateFile({ 'id': wiki_images_7z_id }).GetContentFile('./data/wiki/images.7z')\n",
        "drive.CreateFile({ 'id': wiki_landmarks_7z_id }).GetContentFile('./data/wiki/landmarks.7z')\n",
        "  \n",
        "print('unzipping data...')\n",
        "\n",
        "!rm -rf ./sample_data\n",
        "!cd ./data/utk && p7zip -d ./images.7z >> ../../utk-images.unzip.txt\n",
        "!cd ./data/utk && p7zip -d ./landmarks.7z >> ../../utk-landmarks.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./images.7z >> ../../appareal-images.unzip.txt\n",
        "!cd ./data/appareal && p7zip -d ./landmarks.7z >> ../../appareal-landmarks.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./images.7z >> ../../wiki-images.unzip.txt\n",
        "!cd ./data/wiki && p7zip -d ./landmarks.7z >> ../../wiki-landmarks.unzip.txt\n",
        "!rm -rf *.unzip.txt\n",
        "print('done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QY0yDy-HnN79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "yeObtjLNxBqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "lRTsXamEHK9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!rm -rf /usr/local/lib/python3.6/dist-packages/colabsnippets*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rm3eajF2xAx_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/justadudewhohacks/image_augment.py\n",
        "!pip install git+https://github.com/justadudewhohacks/colabsnippets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYpD_pQBvv7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "WbaNcbk6vr-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import google.colab as colab\n",
        "import tensorflow as tf\n",
        "from augment.augment import augment\n",
        "from colabsnippets import BatchLoader, NeuralNetwork\n",
        "from colabsnippets.utils import load_json "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xj5k6dBZj08I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "lbICGURqj2ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_norm(x, name):\n",
        "  with tf.variable_scope(name):\n",
        "    return tf.nn.batch_normalization(x, tf.get_variable('mean'), tf.get_variable('variance'), tf.get_variable('offset'), tf.get_variable('scale'), 1e-3)\n",
        "\n",
        "def conv2d(x, name, stride, with_batch_norm = False):\n",
        "  with tf.variable_scope(name):\n",
        "    out = tf.nn.conv2d(x, tf.get_variable('filter'), stride, 'SAME')\n",
        "    out = batch_norm(out, 'batch_norm') if with_batch_norm else tf.add(out, tf.get_variable('bias'))\n",
        "    return out\n",
        "\n",
        "def depthwise_separable_conv2d(x, name, stride, with_batch_norm = False):\n",
        "  with tf.variable_scope(name):\n",
        "    out = tf.nn.separable_conv2d(x, tf.get_variable('depthwise_filter'), tf.get_variable('pointwise_filter'), stride, 'SAME')\n",
        "    out = batch_norm(out, weights['batch_norm']) if with_batch_norm else tf.add(out, tf.get_variable('bias'))\n",
        "    return out\n",
        "  \n",
        "def fully_connected(x, name):\n",
        "  with tf.variable_scope(name):\n",
        "    weights = tf.get_variable('weights')\n",
        "    out = tf.reshape(x, [-1, weights.get_shape().as_list()[0]])\n",
        "    out = tf.matmul(out, weights)\n",
        "    out = tf.add(out, tf.get_variable('bias'))\n",
        "  return out\n",
        "\n",
        "def dense_block(x, name, is_first_layer = False, is_scale_down = True, use_depthwise_separable_conv2d = True, with_batch_norm = False):\n",
        "  conv_op = depthwise_separable_conv2d if use_depthwise_separable_conv2d else conv2d\n",
        "  initial_stride = [1, 2, 2, 1]  if is_scale_down else [1, 1, 1, 1]\n",
        "  \n",
        "  with tf.variable_scope(name):\n",
        "    if is_first_layer: \n",
        "      out1 = conv2d(x, 'conv0', initial_stride, with_batch_norm = with_batch_norm) \n",
        "    else: \n",
        "      out1 = conv_op(x, 'conv0', initial_stride, with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in2 = tf.nn.relu(out1)\n",
        "    out2 = conv_op(in2, 'conv1', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in3 = tf.nn.relu(tf.add(out1, out2))\n",
        "    out3 = conv_op(in3, 'conv2', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    in4 = tf.nn.relu(tf.add(out1, tf.add(out2, out3)))\n",
        "    out4 = conv_op(in4, 'conv3', [1, 1, 1, 1], with_batch_norm = with_batch_norm)\n",
        "\n",
        "    return tf.nn.relu(tf.add(out1, tf.add(out2, tf.add(out3, out4))))\n",
        "\n",
        "def normalize(x, mean_rgb):\n",
        "  r, g, b = mean_rgb\n",
        "  shape = np.append(np.array(x.shape[0:3]), [1])\n",
        "  avg_r = tf.fill(shape, r)\n",
        "  avg_g = tf.fill(shape, g)\n",
        "  avg_b = tf.fill(shape, b)\n",
        "  avg_rgb = tf.concat([avg_r, avg_g, avg_b], 3)\n",
        "\n",
        "  return tf.divide(tf.subtract(x, avg_rgb), 255)\n",
        "\n",
        "\n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "      \n",
        "class Densenet_4_4_FeatureExtractor(NeuralNetwork):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4_feature_extractor', channel_multiplier = 1.0):\n",
        "    super().__init__(self.initialize_weights, name = name)\n",
        "    self.use_depthwise_separable_conv2d = use_depthwise_separable_conv2d\n",
        "    self.with_batch_norm = with_batch_norm\n",
        "    self.channel_multiplier = channel_multiplier\n",
        "  \n",
        "  def _create_dense_block_weight_processor(self, weight_processor):\n",
        "    def _dense_block_weight_processor(cin, cout, name, is_first_layer = False):\n",
        "      return weight_processor.process_dense_block_weights(\n",
        "        cin, \n",
        "        cout, \n",
        "        name, \n",
        "        is_first_layer = is_first_layer, \n",
        "        use_depthwise_separable_conv2d = self.use_depthwise_separable_conv2d, \n",
        "        with_batch_norm = self.with_batch_norm\n",
        "      )\n",
        "    \n",
        "    return _dense_block_weight_processor\n",
        "  \n",
        "  def _create_dense_block(self):\n",
        "    def _dense_block(x, name, is_first_layer = False):\n",
        "      return dense_block(\n",
        "        x, \n",
        "        name, \n",
        "        is_first_layer = is_first_layer, \n",
        "        use_depthwise_separable_conv2d = self.use_depthwise_separable_conv2d, \n",
        "        with_batch_norm = self.with_batch_norm\n",
        "      )\n",
        "    \n",
        "    return _dense_block\n",
        "  \n",
        "  def initialize_weights(self, weight_processor, variable_scope = 'feature_extractor'):\n",
        "    process_dense_block_weights = self._create_dense_block_weight_processor(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        c0 = int(self.channel_multiplier * 32)\n",
        "        process_dense_block_weights(3, c0, 'dense0', is_first_layer = True)\n",
        "        process_dense_block_weights(c0, c0 * 2, 'dense1')\n",
        "        process_dense_block_weights(c0 * 2, c0 * 4, 'dense2')\n",
        "        process_dense_block_weights(c0 * 4, c0 * 8, 'dense3')\n",
        "    \n",
        "  def forward(self, batch_tensor, variable_scope = 'feature_extractor'):\n",
        "    dense_block = self._create_dense_block()\n",
        "    \n",
        "    mean_rgb = [122.782, 117.001, 104.298]\n",
        "    normalized = normalize(batch_tensor, mean_rgb)\n",
        "    \n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        out = dense_block(normalized, 'dense0', is_first_layer = True)\n",
        "        out = dense_block(out, 'dense1')\n",
        "        out = dense_block(out, 'dense2')\n",
        "        out = dense_block(out, 'dense3')\n",
        "\n",
        "      return out  \n",
        "  \n",
        "class Densenet_4_5_FeatureExtractor(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_5_feature_extractor'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "    \n",
        "  def initialize_weights(self, weight_processor, variable_scope = 'feature_extractor'):\n",
        "    process_dense_block_weights = super()._create_dense_block_weight_processor(weight_processor)\n",
        "    \n",
        "    super().initialize_weights(weight_processor, variable_scope = variable_scope)\n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        process_dense_block_weights(256, 512, 'dense4')\n",
        "    \n",
        "  def forward(self, batch_tensor, variable_scope = 'feature_extractor'):\n",
        "    dense_block = super()._create_dense_block()\n",
        "    \n",
        "    out = super().forward(batch_tensor, variable_scope = variable_scope)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope(variable_scope):\n",
        "        out = dense_block(out, 'dense4')\n",
        "\n",
        "      return out\n",
        "  \n",
        "class DenseMobilenet_4_4(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4', channel_multiplier = 1.0):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm, \n",
        "      name = name,\n",
        "      channel_multiplier = channel_multiplier\n",
        "    )\n",
        "  \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        c0 = int(self.channel_multiplier * 32)\n",
        "        weight_processor.process_fc_weights(c0 * 8, 1, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "        out = tf.reshape(out, [batch_tensor.shape[0]])\n",
        "\n",
        "    return out\n",
        "  \n",
        "class DenseMobilenet_4_5(Densenet_4_5_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_5'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "    \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    \n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        weight_processor.process_fc_weights(512, 1, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    \n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "        out = tf.reshape(out, [batch_tensor.shape[0]])\n",
        "    \n",
        "    return out\n",
        "  \n",
        "class DenseMobilenet_4_4_DEX(Densenet_4_4_FeatureExtractor):\n",
        "  def __init__(self, use_depthwise_separable_conv2d = True, with_batch_norm = False, name = 'densenet_4_4_dex'):\n",
        "    super().__init__(\n",
        "      use_depthwise_separable_conv2d = use_depthwise_separable_conv2d,\n",
        "      with_batch_norm = with_batch_norm,\n",
        "      name = name\n",
        "    )\n",
        "  \n",
        "  def initialize_weights(self, weight_processor):\n",
        "    super().initialize_weights(weight_processor)\n",
        "    with tf.variable_scope(self.name):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        weight_processor.process_fc_weights(256, 101, 'fc_age')\n",
        "    \n",
        "  def forward(self, batch_tensor):\n",
        "    out = super().forward(batch_tensor)\n",
        "    with tf.variable_scope(self.name, reuse = True):\n",
        "      with tf.variable_scope('classifier'):\n",
        "        out = tf.nn.avg_pool(out, [1, 7, 7, 1], [1, 2, 2, 1], 'VALID')\n",
        "        out = fully_connected(out, 'fc_age')\n",
        "        out = tf.reduce_sum(tf.multiply(tf.nn.softmax(out), np.arange(101)), axis = 1)\n",
        "    \n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-zXGSH-Qw1k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Common"
      ]
    },
    {
      "metadata": {
        "id": "vSRUcrLYQvwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "utility\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "def gpu_session(callback):\n",
        "  config = tf.ConfigProto()\n",
        "  config.gpu_options.allow_growth = True\n",
        "  config.allow_soft_placement = True\n",
        "  config.log_device_placement = True\n",
        "  with tf.Session(config = config) as session:\n",
        "    with tf.device('/gpu:0'):\n",
        "      callback(session)\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "\n",
        "def download_epoch_files(start, end):\n",
        "  for epoch in range(start, end):\n",
        "    colab.files.download('epoch_' + str(epoch) + '.txt')\n",
        "    colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "    colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "    colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "def save_weights(var_list, checkpoint_file):\n",
        "  checkpoint_data = np.array([])\n",
        "  meta_data = []\n",
        "  for var in var_list:\n",
        "    meta_data.append({ 'shape': var.get_shape().as_list(), 'name': var.name })\n",
        "    checkpoint_data = np.append(checkpoint_data, var.eval().flatten())\n",
        "    \n",
        "  meta_json = open(checkpoint_file + '.json', 'w')\n",
        "  meta_json.write(json.dumps(meta_data))\n",
        "  meta_json.close()\n",
        "  np.save(checkpoint_file, checkpoint_data)\n",
        "\n",
        "\n",
        "'''\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "Data Loader\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "appareal_labels = load_json('./data/appareal/labels.json')\n",
        "wiki_labels = load_json('./data/wiki/labels.json')\n",
        "\n",
        "def extract_data_labels(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "\n",
        "  if db == 'utk':\n",
        "    age = int(float(img_file.split('_')[0]))\n",
        "    return age\n",
        "  elif db == 'appareal':\n",
        "    age = appareal_labels[img_file]['age']\n",
        "    return age\n",
        "  elif db == 'wiki':\n",
        "    age = wiki_labels[img_file]['age']\n",
        "    return age\n",
        "  else: raise('unknown db: ' + db)\n",
        "    \n",
        "def resolve_image_path(data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  return './data/' + db + '/cropped-images/' + img_file\n",
        "\n",
        "def min_bbox(landmarks):\n",
        "  min_x, min_y, max_x, max_y = 1.0, 1.0, 0, 0\n",
        "  for pt in landmarks:\n",
        "    min_x = pt['x'] if pt['x'] < min_x else min_x\n",
        "    min_y = pt['y'] if pt['y'] < min_y else min_y\n",
        "    max_x = max_x if pt['x'] < max_x else pt['x']\n",
        "    max_y = max_y if pt['y'] < max_y else pt['y']\n",
        "\n",
        "  return [min_x, min_y, max_x, max_y]\n",
        "\n",
        "def augment_image(img, data):\n",
        "  db = data['db']\n",
        "  img_file = data['file']\n",
        "  file_suffix = 'chip_0' if db == 'utk' else ('face_0' if db == 'appareal' else '')\n",
        "  landmarks_file = img_file.replace(file_suffix + '.jpg', file_suffix + '.json')\n",
        "  landmarks_file_path = './data/' + db + '/landmarks/' + landmarks_file\n",
        "\n",
        "  landmarks = load_json(landmarks_file_path)\n",
        "  \n",
        "  \n",
        "  intensity_config = { 'alpha': random.uniform(0.5, 1.5), 'beta': random.uniform(-20, 20) }\n",
        "  blur_config = { 'kernel_size':  random.choice([0, 3, 5, 7, 9, 11]), 'std_dev': random.uniform(0.5, 1.5) }\n",
        "  blur_prob = 0.5\n",
        "  flip_prob = 0.5\n",
        "  gray_prob = 0.2\n",
        "\n",
        "  return augment(\n",
        "    img,\n",
        "    random_crop = min_bbox(landmarks),\n",
        "    flip = random.random() < flip_prob,\n",
        "    rotation_angle = random.uniform(-15, 15),\n",
        "    shear = [random.uniform(0.0, 0.2), random.uniform(0.0, 0.2)],\n",
        "    stretch = { 'stretch_x': random.uniform(1.0, 1.4), 'stretch_y': random.uniform(1.0, 1.4) },\n",
        "    intensity = intensity_config,\n",
        "    blur = blur_config if random.random() < blur_prob else None,\n",
        "    hsv = [random.uniform(-5, 5), random.uniform(-15, 15), random.uniform(-20, 20)],\n",
        "    to_gray = random.random() < gray_prob\n",
        "    #rotation_angle = random.uniform(-5, 5),\n",
        "    #blur = { 'kernel_size':  random.choice([3, 5, 7]), 'std_dev': random.uniform(0.8, 1.2) },\n",
        "    #intensity = { 'alpha': random.uniform(0.8, 1.2), 'beta': random.uniform(-10, 10) },\n",
        "    #to_gray = random.random() < 0.1\n",
        "  )\n",
        "\n",
        "class DataLoader(BatchLoader):\n",
        "  def __init__(self, data, with_augmentation = False, start_epoch = None, is_test = False):\n",
        "    BatchLoader.__init__(\n",
        "      self, \n",
        "      data, \n",
        "      resolve_image_path, \n",
        "      extract_data_labels, \n",
        "      augment_image = augment_image if with_augmentation else None, \n",
        "      start_epoch = start_epoch, \n",
        "      is_test = is_test\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZ_9nRRuoHuF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "p4S3cqgioJTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4(channel_multiplier = 2.0)\n",
        "model_name = './dense_mobilenet_4_4_lg_augmented3'\n",
        "image_size = 112\n",
        "\n",
        "# training parameters\n",
        "learning_rate = 0.001\n",
        "start_epoch = 0\n",
        "end_epoch = 2000\n",
        "batch_size = 32\n",
        "\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size])\n",
        "\n",
        "train_data = load_json('./data/trainData.json')\n",
        "data_loader = DataLoader(train_data, start_epoch = start_epoch, with_augmentation = True)\n",
        "net.init_trainable_weights()\n",
        "\n",
        "#train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"densenet_4_4_dex/classifier\") \n",
        "#print(train_vars)\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)#, var_list = train_vars)\n",
        "\n",
        "log_file = open('./log.txt', 'w')\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "  \n",
        "def train(sess):\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_epoch = time.time()\n",
        "  \n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  if (start_epoch != 0):\n",
        "    checkpoint = get_checkpoint(start_epoch - 1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    print('done restoring session')\n",
        "\n",
        "  while data_loader.epoch <= end_epoch:\n",
        "    epoch = data_loader.epoch\n",
        "    current_idx = data_loader.current_idx\n",
        "    end_idx = data_loader.get_end_idx()\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    batch_x, batch_y = data_loader.next_batch(batch_size, image_size)\n",
        "\n",
        "    loss, _ = sess.run([loss_op, train_op], feed_dict = { X: batch_x, Y: batch_y })\n",
        "    total_loss += loss\n",
        "    iteration_count += 1\n",
        "    log_file.write(\"epoch \" + str(epoch) + \", (\" + str(current_idx) + \" of \" + str(end_idx) + \"), loss= \" + \"{:.4f}\".format(loss) \n",
        "          + \", time= \" + str((time.time() - ts) * 1000) + \"ms \\n\")\n",
        "\n",
        "    if epoch != data_loader.epoch:\n",
        "      print('next epoch: ' + str(data_loader.epoch))\n",
        "      print('avg_loss= ' + str(total_loss / iteration_count))\n",
        "      saver.save(sess, model_name + '.ckpt', global_step = epoch)\n",
        "\n",
        "      epoch_txt_file_path = 'epoch_' + str(epoch) + '.txt'\n",
        "      epoch_txt = open(epoch_txt_file_path, 'w')\n",
        "      epoch_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "      epoch_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "      epoch_txt.write('learning_rate= ' + str(learning_rate) + '\\n')\n",
        "      epoch_txt.write('batch_size= ' + str(batch_size) + '\\n')\n",
        "      epoch_txt.write('epoch_time= ' + str(time.time() - ts_epoch) + 's \\n')\n",
        "      epoch_txt.close()\n",
        "\n",
        "      #colab.files.download(epoch_txt_file_path)\n",
        "      #colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "      total_loss = 0\n",
        "      iteration_count = 0              \n",
        "      ts_epoch = time.time()\n",
        "        \n",
        "  print('done!')\n",
        "  log_file.close() \n",
        "    \n",
        "gpu_session(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDQqQSTTbodd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "JkzRXmBJbraO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.allow_soft_placement = True\n",
        "config.log_device_placement = True\n",
        "\n",
        "net = DenseMobilenet_4_4(channel_multiplier = 2.0)\n",
        "model_name = './dense_mobilenet_4_4_lg_augmented3'\n",
        "\n",
        "def get_checkpoint(epoch):\n",
        "  return model_name + '.ckpt-' + str(epoch)\n",
        "\n",
        "def compile_loss_op(X, Y):\n",
        "  age = net.forward(X)\n",
        "  loss_op = tf.losses.absolute_difference(age, Y)\n",
        "  return loss_op\n",
        "\n",
        "batch_size = 32\n",
        "dbs = ['utk', 'wiki', 'appareal']\n",
        "test_data = load_json('./data/testData.json')\n",
        "\n",
        "for epoch in range(148, -1, -1):\n",
        "  tf.reset_default_graph()\n",
        "  net.init_trainable_weights()\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "  Y = tf.placeholder(tf.float32, [batch_size])\n",
        "  loss_op = compile_loss_op(X, Y)\n",
        "\n",
        "  test_txt = open('test_epoch_' + str(epoch) + '.txt', 'w')\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "  saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_test = time.time()\n",
        "  #with tf.Session(tpu_address) as sess:\n",
        "  with tf.Session(config = config) as sess:\n",
        "    checkpoint = get_checkpoint(epoch)\n",
        "    sess.run(init)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    with tf.device('/gpu:0'):\n",
        "\n",
        "      total_loss_db = 0\n",
        "      iteration_count_db = 0\n",
        "      for db in dbs:\n",
        "        db_data = []\n",
        "        for data in test_data:\n",
        "          if data['db'] == db:\n",
        "            db_data.append(data)\n",
        "\n",
        "        data_loader = DataLoader(db_data, is_test = True)\n",
        "        next_batch = data_loader.next_batch(batch_size)\n",
        "        while next_batch != None:\n",
        "          batch_x, batch_y = next_batch\n",
        "          if batch_x.shape[0] != batch_size:\n",
        "            X_tmp = tf.placeholder(tf.float32, [batch_x.shape[0], 112, 112, 3])\n",
        "            Y_tmp = tf.placeholder(tf.float32, [batch_x.shape[0]])\n",
        "            loss_op_tmp = compile_loss_op(X_tmp, Y_tmp)\n",
        "            loss = sess.run(loss_op_tmp, feed_dict = { X_tmp: batch_x, Y_tmp: batch_y }) #/ batch_x.shape[0]\n",
        "          else:\n",
        "            loss = sess.run(loss_op, feed_dict = { X: batch_x, Y: batch_y }) #/ batch_size\n",
        "          total_loss += loss\n",
        "          total_loss_db += loss\n",
        "          iteration_count += 1\n",
        "          iteration_count_db += 1\n",
        "          next_batch = data_loader.next_batch(batch_size)\n",
        "\n",
        "        print(str(db) + \", avg_loss= \" + str(total_loss_db / iteration_count_db))\n",
        "        test_txt.write(str(db) + \":\" + '\\n')\n",
        "        test_txt.write('total_loss= ' + str(total_loss_db) + '\\n')\n",
        "        test_txt.write('avg_loss= ' + str(total_loss_db / iteration_count_db) + '\\n')\n",
        "        test_txt.write('\\n')\n",
        "        total_loss_db = 0\n",
        "        iteration_count_db = 0\n",
        "\n",
        "      print(\"avg_loss= \" + str(total_loss / iteration_count))\n",
        "      test_txt.write('----------------\\n\\n')\n",
        "      test_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "      test_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "      test_txt.write('test_time= ' + str(time.time() - ts_test) + 's \\n')\n",
        "      test_txt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A7NA2g2fbEYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overfit"
      ]
    },
    {
      "metadata": {
        "id": "QhRQWkbsbB0_",
        "colab_type": "code",
        "outputId": "b68bd4a6-5f45-460d-a7b8-a6468d07dc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4()\n",
        "model_name = './dense_mobilenet_4_4'\n",
        "image_size = 112\n",
        "\n",
        "# training parameters\n",
        "learning_rate = 0.001\n",
        "start_epoch = 1\n",
        "end_epoch = 2000\n",
        "batch_size = 16\n",
        "\n",
        "#optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [batch_size])\n",
        "\n",
        "train_data = load_json('./data/trainData.json')[0:batch_size]\n",
        "data_loader = DataLoader(train_data, start_epoch = start_epoch)\n",
        "net.init_trainable_weights()\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "log_file = open('./log.txt', 'w')\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "def overfit(sess):\n",
        "  total_loss = 0\n",
        "  iteration_count = 0\n",
        "  ts_epoch = time.time()\n",
        "\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  if (start_epoch != 0):\n",
        "    checkpoint = get_checkpoint(start_epoch - 1)\n",
        "    saver.restore(sess, checkpoint)\n",
        "    print('done restoring session')\n",
        " \n",
        "  while data_loader.epoch <= end_epoch:\n",
        "    epoch = data_loader.epoch\n",
        "    current_idx = data_loader.current_idx\n",
        "    end_idx = data_loader.get_end_idx()\n",
        "\n",
        "    ts = time.time()\n",
        "\n",
        "    batch_x, batch_y = data_loader.next_batch(batch_size, image_size)\n",
        "    loss, _ = sess.run([loss_op, train_op], feed_dict = { X: batch_x, Y: batch_y })\n",
        "    total_loss += loss\n",
        "    iteration_count += 1\n",
        "    log_file.write(\"epoch \" + str(epoch) + \", (\" + str(current_idx) + \" of \" + str(end_idx) + \"), loss= \" + \"{:.4f}\".format(loss) \n",
        "          + \", time= \" + str((time.time() - ts) * 1000) + \"ms \\n\")\n",
        "\n",
        "    if epoch != data_loader.epoch:\n",
        "      print(str(data_loader.epoch) + ': ' + \"{:.4f}\".format(total_loss / iteration_count) + \", \" + \"{:.4f}\".format(total_loss))\n",
        "      if False:\n",
        "        print('next epoch: ' + str(data_loader.epoch))\n",
        "        saver.save(sess, model_name + '.ckpt', global_step = epoch)\n",
        "\n",
        "        epoch_txt_file_path = 'epoch_' + str(epoch) + '.txt'\n",
        "        epoch_txt = open(epoch_txt_file_path, 'w')\n",
        "        epoch_txt.write('total_loss= ' + str(total_loss) + '\\n')\n",
        "        epoch_txt.write('avg_loss= ' + str(total_loss / iteration_count) + '\\n')\n",
        "        epoch_txt.write('learning_rate= ' + str(learning_rate) + '\\n')\n",
        "        epoch_txt.write('batch_size= ' + str(batch_size) + '\\n')\n",
        "        epoch_txt.write('epoch_time= ' + str(time.time() - ts_epoch) + 's \\n')\n",
        "        epoch_txt.close()\n",
        "\n",
        "      #colab.files.download(epoch_txt_file_path)\n",
        "      #colab.files.download(get_checkpoint(epoch) + '.index') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.meta') \n",
        "      #colab.files.download(get_checkpoint(epoch) + '.data-00000-of-00001')\n",
        "\n",
        "      total_loss = 0\n",
        "      iteration_count = 0              \n",
        "      ts_epoch = time.time()\n",
        "\n",
        "  print('done!')\n",
        "  log_file.close() \n",
        "  \n",
        "gpu_session(overfit)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-41843d1dc267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/trainData.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_trainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "WxtFV1RyTl5U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transfer Weights"
      ]
    },
    {
      "metadata": {
        "id": "U_-OqBL6x9VG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract Feature Extractor Weights"
      ]
    },
    {
      "metadata": {
        "id": "l4fro9I-plOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4()\n",
        "net.load_weights('./ref')\n",
        "\n",
        "def run1(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  save_weights(tf.global_variables()[:len(tf.global_variables()) - 2], './feature_extractor')\n",
        "\n",
        "gpu_session(run1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-F3HIJUsyILD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Feature Extractor Weights and Initialize Classifier Weights"
      ]
    },
    {
      "metadata": {
        "id": "3uKIzch6Jr8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "net.load_weights('./feature_extractor')\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = 100)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [32, 112, 112, 3])\n",
        "Y = tf.placeholder(tf.float32, [32])\n",
        "\n",
        "age = net.forward(X)\n",
        "loss_op = tf.losses.absolute_difference(age, Y)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "print(len(tf.global_variables()))\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "def run1(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.save(sess, 'dense_mobilenet_4_4_dex.ckpt-0')\n",
        "\n",
        "gpu_session(run1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrfRRrHuyZuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Debug"
      ]
    },
    {
      "metadata": {
        "id": "t0_NxMGlJJYN",
        "colab_type": "code",
        "outputId": "8041bd3e-6ddc-42f1-c9d5-f465e94ed26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "src_net = DenseMobilenet_4_4()\n",
        "src_net.load_weights('./feature_extractor')\n",
        "\n",
        "vars1 = []\n",
        "\n",
        "def run1(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for var in tf.global_variables():\n",
        "    vars1.append(var.eval())\n",
        "\n",
        "gpu_session(run1)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "net = DenseMobilenet_4_4_DEX()\n",
        "net.init_trainable_weights()\n",
        "saver = tf.train.Saver(max_to_keep = None)\n",
        "\n",
        "vars2 = []\n",
        "\n",
        "def run2(sess):\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  saver.restore(sess, 'dense_mobilenet_4_4_dex.ckpt-0')\n",
        "  for var in tf.global_variables():\n",
        "    vars2.append(var.eval())\n",
        "  \n",
        "gpu_session(run2) \n",
        "\n",
        "print(np.array(vars1[0]) - np.array(vars2[0]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load_weights - warning meta_json does not contain data for idx: 47, using default initializer\n",
            "load_weights - warning meta_json does not contain data for idx: 48, using default initializer\n",
            "INFO:tensorflow:Restoring parameters from dense_mobilenet_4_4_dex.ckpt-0\n",
            "[[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}